[
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "\"Must Be a Tuesday\": Affect, Attribution, and Geographic Variability in Equity-Oriented Visualizations of Population Health Disparities",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00021",
    "authors": [
      {
        "name": "Eli Holder",
        "affiliation": "N/A"
      },
      {
        "name": "Lace M. Padilla",
        "affiliation": "N/A"
      }
    ],
    "abstract": "This study examines the impacts of public health communications visualizing risk disparities between racial and other social groups. It compares the effects of traditional bar charts to an alternative design emphasizing geographic variability with differing annotations and jitter plots. Whereas both visualization designs increased perceived vulnerability, behavioral intent, and policy support, the geo-emphasized charts were significantly more effective in reducing personal attribution biases. The findings also reveal emotionally taxing experiences for chart viewers from marginalized communities. This work suggests a need for strategic reevaluation of visual communication tools in public health to enhance understanding and engagement without reinforcing stereotypes or emotional distress.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "A Comparative Study of Neural Surface Reconstruction for Scientific Visualization",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00045",
    "authors": [
      {
        "name": "Siyuan Yao",
        "affiliation": "University of Notre Dame"
      },
      {
        "name": "Weixi Song",
        "affiliation": "Wuhan University"
      },
      {
        "name": "Chaoli Wang",
        "affiliation": "University of Notre Dame"
      }
    ],
    "abstract": "This comparative study evaluates various neural surface reconstruction methods, particularly focusing on their implications for scientific visualization through reconstructing 3D surfaces via multi-view rendering images. We categorize ten methods into neural radiance fields and neural implicit surfaces, uncovering the benefits of leveraging distance functions (i.e., SDFs and UDFs) to enhance the accuracy and smoothness of the reconstructed surfaces. Our findings highlight the efficiency and quality of NeuS2 for reconstructing closed surfaces and identify NeUDF as a promising candidate for reconstructing open surfaces despite some limitations. By sharing our benchmark dataset, we invite researchers to test the performance of their methods, contributing to the advancement of surface reconstruction solutions for scientific visualization.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "A Literature-based Visualization Task Taxonomy for Gantt Charts",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00055",
    "authors": [
      {
        "name": "Sayef Azad Sakin",
        "affiliation": "SCI Institute and Kahlert School of Computing, The University of Utah"
      },
      {
        "name": "Katherine E. Isaacs",
        "affiliation": "SCI Institute and Kahlert School of Computing, The University of Utah"
      }
    ],
    "abstract": "Gantt charts are a widely-used idiom for visualizing temporal discrete event sequence data where dependencies exist between events. They are popular in domains such as manufacturing and computing for their intuitive layout of such data. However, these domains frequently generate data at scales which tax both the visual representation and the ability to render it at interactive speeds. To aid visualization developers who use Gantt charts in these situations, we develop a task taxonomy of low level visualization tasks supported by Gantt charts and connect them to the data queries needed to support them. Our taxonomy is derived through a literature survey of visualizations using Gantt charts over the past 30 years.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "A Ridge-based Approach for Extraction and Visualization of 3D Atmospheric Fronts",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00043",
    "authors": [
      {
        "name": "Anne Gossing",
        "affiliation": "Zuse Institute Berlin"
      },
      {
        "name": "Andreas Beckert",
        "affiliation": "Universit\u00e4t Hamburg"
      },
      {
        "name": "Christoph Fischer",
        "affiliation": "Universit\u00e4t Hamburg"
      },
      {
        "name": "Nicolas Klenert",
        "affiliation": "Zuse Institute Berlin"
      },
      {
        "name": "Vijay Natarajan",
        "affiliation": "Indian Institute of Science Bangalore"
      },
      {
        "name": "George Pacey",
        "affiliation": "Freie Universit\u00e4t Berlin"
      },
      {
        "name": "Thorwin Vogt",
        "affiliation": "Universit\u00e4t Hamburg"
      },
      {
        "name": "Marc Rautenhaus",
        "affiliation": "Universit\u00e4t Hamburg"
      },
      {
        "name": "Daniel Baum",
        "affiliation": "Zuse Institute Berlin"
      }
    ],
    "abstract": "An atmospheric front is an imaginary surface that separates two distinct air masses and is commonly defined as the warm-air side of a frontal zone with high gradients of atmospheric temperature and humidity (Fig. 1, left). These fronts are a widely used conceptual model in meteorology, which are often encountered in the literature as two-dimensional (2D) front lines on surface analysis charts. This paper presents a method for computing three-dimensional (3D) atmospheric fronts as surfaces that is capable of extracting continuous and well-confined features suitable for 3D visual analysis, spatiotemporal tracking, and statistical analyses (Fig. 1, middle, right). Recently developed contour-based methods for 3D front extraction rely on computing the third derivative of a moist potential temperature field. Additionally, they require the field to be smoothed to obtain continuous large-scale structures. This paper demonstrates the feasibility of an alternative method to front extraction using ridge surface computation. The proposed method requires only the second derivative of the input field and produces accurate structures even from unsmoothed data. An application of the ridge-based method to a data set corresponding to Cyclone Friederike demonstrates its benefits and utility towards visual analysis of the full 3D structure of fronts.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "A Simple yet Useful Spiral Visualization of Large Graphs",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00043",
    "authors": [
      {
        "name": "Garima Jindal",
        "affiliation": "International Institute of Information Technology, Hyderabad, India"
      },
      {
        "name": "Kamalakar Karlapalem",
        "affiliation": "International Institute of Information Technology, Hyderabad, India"
      }
    ],
    "abstract": "We present a Spiral Visualization that facilitates users to visually comprehend large graphs. Spiral Visualization is a representation that highlights key aspects of networks, including the number, size, and density of communities, important or central nodes within communities, centrality distribution within communities, connections between communities, and connections between nodes. To facilitate analysis and comprehension of networks using various interaction techniques, such as zooming, tooltip, and highlight, we have implemented a Spiral Visualization dashboard. We conducted a qualitative user study incorporating observation, think-aloud protocols, and participant rating of confidence and easiness to assess the usability and suitability of our visualization. The findings suggest that our visualization is appropriate for the network tasks evaluated. However, tasks requiring color comparison, such as identifying the densest community and comparing community densities were found to be more challenging to perform.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "A Two-Phase Visualization System for Continuous Human-AI Collaboration in Sequelae Analysis and Modeling",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00028",
    "authors": [
      {
        "name": "Yang Ouyang",
        "affiliation": "ShanghaiTech University"
      },
      {
        "name": "Chenyang Zhang",
        "affiliation": "University of Illinois at Urbana-Champaign"
      },
      {
        "name": "He Wang",
        "affiliation": "ShanghaiTech University"
      },
      {
        "name": "Tianle Ma",
        "affiliation": "Zhongshan Hospital Fudan University"
      },
      {
        "name": "Chang Jiang",
        "affiliation": "Zhongshan Hospital Fudan University"
      },
      {
        "name": "Yuheng Yan",
        "affiliation": "ShanghaiTech University"
      },
      {
        "name": "Zuoqin Yan",
        "affiliation": "Zhongshan Hospital Fudan University"
      },
      {
        "name": "Xiaojuan Ma",
        "affiliation": "Hong Kong University of Science and Technology"
      },
      {
        "name": "Chuhan Shi",
        "affiliation": "Southeast University"
      },
      {
        "name": "Quan Li",
        "affiliation": "ShanghaiTech University"
      }
    ],
    "abstract": "In healthcare, AI techniques are widely used for tasks like risk assessment and anomaly detection. Despite AI\u2019s potential as a valuable assistant, its role in complex medical data analysis often over-simplifies human-AI collaboration dynamics. To address this, we collaborated with a local hospital, engaging six physicians and one data scientist in a formative study. From this collaboration, we propose a framework integrating two-phase interactive visualization systems: one for Human-Led, AI-Assisted Retrospective Analysis and another for AI-Mediated, Human-Reviewed Iterative Modeling. This framework aims to enhance understanding and discussion around effective human-AI collaboration in healthcare.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "A Visualization System for Hexahedral Mesh Quality Study",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00026",
    "authors": [
      {
        "name": "Lei Si",
        "affiliation": "University of Houston"
      },
      {
        "name": "Guoning Chen",
        "affiliation": "University of Houston"
      }
    ],
    "abstract": "In this paper, we introduce a new 3D hex mesh visual analysis system that emphasizes poor-quality areas with an aggregated glyph, highlights overlapping elements, and provides detailed boundary error inspection in three forms. By supporting multi-level analysis through multiple views, our system effectively evaluates various mesh models and compares the performance of mesh generation and optimization algorithms for hexahedral meshes.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "AEye: A Visualization Tool for Image Datasets",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00064",
    "authors": [
      {
        "name": "Florian Gr\u00f6tschla",
        "affiliation": "ETH Zurich, Switzerland"
      },
      {
        "name": "Luca A. Lanzend\u00f6rfer",
        "affiliation": "ETH Zurich, Switzerland"
      },
      {
        "name": "Marco Calzavara",
        "affiliation": "ETH Zurich, Switzerland"
      },
      {
        "name": "Roger Wattenhofer",
        "affiliation": "ETH Zurich, Switzerland"
      }
    ],
    "abstract": "Image datasets serve as the foundation for machine learning models in computer vision, significantly influencing model capabilities, performance, and biases alongside architectural considerations. Therefore, understanding the composition and distribution of these datasets has become increasingly crucial. To address the need for intuitive exploration of these datasets, we propose AEye, an extensible and scalable visualization tool tailored to image datasets. AEye utilizes a contrastively trained model to embed images into semantically meaningful high-dimensional representations, facilitating data clustering and organization. To visualize the high-dimensional representations, we project them onto a two-dimensional plane and arrange images in layers so users can seamlessly navigate and explore them interactively. AEye facilitates semantic search functionalities for both text and image queries, enabling users to search for content. We open-source the codebase for AEye, and provide a simple configuration to add datasets.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Accelerating Transfer Function Update for Distance Map based Volume Rendering",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00042",
    "authors": [
      {
        "name": "Michael Rauter",
        "affiliation": "Competence Center for Preclinical Imaging and Biomedical Engineering, Faculty of Health, University of Applied Sciences Wiener Neustadt, Austria"
      },
      {
        "name": "Lukas Zimmermann",
        "affiliation": "Department of Radiation Oncology, Medical University of Vienna, Austria"
      },
      {
        "name": "Markus Zeilinger",
        "affiliation": "Competence Center for Preclinical Imaging and Biomedical Engineering, Faculty of Health, University of Applied Sciences Wiener Neustadt, Austria"
      }
    ],
    "abstract": "Direct volume rendering using ray-casting is widely used in practice. By using GPUs and applying acceleration techniques as empty space skipping, high frame rates are possible on modern hardware. This enables performance-critical use-cases such as virtual reality volume rendering. The currently fastest known technique uses volumetric distance maps to skip empty sections of the volume during ray-casting but requires the distance map to be updated per transfer function change. In this paper, we demonstrate a technique for subdividing the volume intensity range into partitions and deriving what we call partitioned distance maps. These can be used to accelerate the distance map computation for a newly changed transfer function by a factor up to 30. This allows the currently fastest known empty space skipping approach to be used while maintaining high frame rates even when the transfer function is changed frequently.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Active Appearance and Spatial Variation Can Improve Visibility in Area Labels for Augmented Reality",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00049",
    "authors": [
      {
        "name": "Hojung Kwon",
        "affiliation": "Brown University"
      },
      {
        "name": "Yuanbo Li",
        "affiliation": "Brown University"
      },
      {
        "name": "Xiaohan Ye",
        "affiliation": "Brown University"
      },
      {
        "name": "Praccho Muna-McQuay",
        "affiliation": "Brown University"
      },
      {
        "name": "Liuren Yin",
        "affiliation": "Brown University"
      },
      {
        "name": "James Tompkin",
        "affiliation": "Brown University"
      }
    ],
    "abstract": "Augmented reality (AR) area labels can visualize real world regions with arbitrary boundaries and show invisible objects or features. But environment conditions such as lighting and clutter can decrease fixed or passive label visibility, and labels that have high opacity levels can occlude crucial details in the environment. We design and evaluate active AR area label visualization modes to enhance visibility across real-life environments, while still retaining environment details within the label. For this, we define a distant characteristic color from the environment in perceptual CIELAB space, then introduce spatial variations among label pixel colors based on the underlying environment variation. In a user study with 18 participants, we found that our active label visualization modes can be comparable in visibility to a fixed green baseline by Gabbard et al., and can outperform it with added spatial variation in cluttered environments, across varying levels of lighting (e.g., nighttime), and in environments with colors similar to the fixed baseline color.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "AltGeoViz: Facilitating Accessible Geovisualization",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00020",
    "authors": [
      {
        "name": "Chu Li",
        "affiliation": "University of Washington"
      },
      {
        "name": "Rock Yuren Pang",
        "affiliation": "University of Washington"
      },
      {
        "name": "Ather Sharif",
        "affiliation": "University of Washington"
      },
      {
        "name": "Arnavi Chheda-Kothary",
        "affiliation": "University of Washington"
      },
      {
        "name": "Jeffrey Heer",
        "affiliation": "University of Washington"
      },
      {
        "name": "Jon E. Froehlich",
        "affiliation": "University of Washington"
      }
    ],
    "abstract": "Geovisualizations are powerful tools for exploratory spatial analysis, enabling sighted users to discern patterns, trends, and relationships within geographic data. However, these visual tools have remained largely inaccessible to screen-reader users. We introduce AltGeoViz, a new interactive geovisualization approach that dynamically generates alt-text descriptions based on the user\u2019s current map view, providing voiceover summaries of spatial patterns and descriptive statistics. In a remote user study with five screen-reader users, we found that participants were able to interact with spatial data in previously infeasible ways, demonstrated a clear understanding of data summaries and their location context, and could synthesize spatial understandings of their explorations. Moreover, we identified key areas for improvement, such as the addition of spatial navigation controls and comparative analysis features.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "An Overview+Detail Layout for Visualizing Compound Graphs",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00035",
    "authors": [
      {
        "name": "Chang Han",
        "affiliation": "The University of Utah"
      },
      {
        "name": "Justin Lieffers",
        "affiliation": "The University of Arizona"
      },
      {
        "name": "Clayton Morrison",
        "affiliation": "Lum AI"
      },
      {
        "name": "Katherine E. Isaacs",
        "affiliation": "The University of Utah"
      }
    ],
    "abstract": "Compound graphs are networks in which vertices can be grouped into larger subsets, with these subsets capable of further grouping, resulting in a nesting that can be many levels deep. In several applications, including biological workflows, chemical equations, and computational data flow analysis, these graphs often exhibit a tree-like nesting structure, where sibling clusters are disjoint. Common compound graph layouts prioritize the lowest level of the grouping, down to the individual ungrouped vertices, which can make the higher level grouped structures more difficult to discern, especially in deeply nested networks. Leveraging the additional structure of the tree-like nesting, we contribute an overview+detail layout for this class of compound graphs that preserves the saliency of the higher level network structure when groups are expanded to show internal nested structure. Our layout draws inner structures adjacent to their parents, using a modified tree layout to place substructures. We describe our algorithm and then present case studies demonstrating the layout's utility to a domain expert working on data flow analysis. Finally, we discuss network parameters and analysis situations in which our layout is well suited.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Animating the Narrative: A Review of Animation Styles in Narrative Visualization",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00074",
    "authors": [
      {
        "name": "Vyri Yang",
        "affiliation": "Louisiana State University, Baton Rouge, Louisiana, United States"
      },
      {
        "name": "Mahmood Jasim",
        "affiliation": "Louisiana State University, Baton Rouge, Louisiana, United States"
      }
    ],
    "abstract": "Narrative visualization has become a crucial tool in data presentation, merging storytelling with data visualization to convey complex information in an engaging and accessible manner. In this study, we review the design space for narrative visualizations, focusing on animation style, through a comprehensive analysis of 80 papers from key visualization venues. We categorize these papers into six broad themes: Animation Style, Interactivity, Technology Usage, Methodology Development, Evaluation Type, and Application Domain. Our findings reveal a significant evolution in the field, marked by a growing preference for animated and non-interactive techniques. This trend reflects a shift towards minimizing user interaction while enhancing the clarity and impact of data presentation. We also identified key trends and technologies shaping the field, highlighting the role of technologies, such as machine learning in driving these changes. We offer insights into the dynamic interrelations within the narrative visualization domains, and suggest future research directions, including exploring non-interactive techniques, examining the interplay between different visualization elements, and developing domain-specific visualizations.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Assessing Graphical Perception of Image Embedding Models using Channel Effectiveness",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00053",
    "authors": [
      {
        "name": "Soohyun Lee",
        "affiliation": "Seoul National University"
      },
      {
        "name": "Minsuk Chang",
        "affiliation": "Seoul National University"
      },
      {
        "name": "Seokhyeon Park",
        "affiliation": "Seoul National University"
      },
      {
        "name": "Jinwook Seo",
        "affiliation": "Seoul National University"
      }
    ],
    "abstract": "Recent advancements in vision models have greatly improved their ability to handle complex chart understanding tasks, like chart captioning and question answering. However, it remains challenging to assess how these models process charts. Existing benchmarks only roughly evaluate model performance without evaluating the underlying mechanisms, such as how models extract image embeddings. This limits our understanding of the model\u2019s ability to perceive fundamental graphical components. To address this, we introduce a novel evaluation framework to assess the graphical perception of image embedding models. For chart comprehension, we examine two main aspects of channel effectiveness: accuracy and discriminability of various visual channels. Channel accuracy is assessed through the linearity of embeddings, measuring how well the perceived magnitude aligns with the size of the stimulus. Discrim-inability is evaluated based on the distances between embeddings, indicating their distinctness. Our experiments with the CLIP model show that it perceives channel accuracy differently from humans and shows unique discriminability in channels like length, tilt, and curvature. We aim to develop this work into a broader benchmark for reliable visual encoders, enhancing models for precise chart comprehension and human-like perception in future applications.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Augmented Reality as a Visualization Technique for Scholarly Publications in Astronomy: An Empirical Evaluation",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00016",
    "authors": [
      {
        "name": "Jane L. Adams",
        "affiliation": "Northeastern University"
      },
      {
        "name": "Laura South",
        "affiliation": "Northeastern University"
      },
      {
        "name": "Arzu \u00c7\u00f6ltekin",
        "affiliation": "University of Applied Sciences and Arts, Northwestern Switzerland (FHNW)"
      },
      {
        "name": "Alyssa A. Goodman",
        "affiliation": "Harvard University"
      },
      {
        "name": "Michelle A. Borkin",
        "affiliation": "Northeastern University"
      }
    ],
    "abstract": "We present a mixed methods user study evaluating augmented reality (AR) as a visualization technique for use in astronomy journal publications. This work is motivated by the highly spatial nature of scientific visualizations employed in astronomy, including spatial reasoning tasks for hypothesis generation and scientific communications. In this 52-person user study, we evaluate two AR approaches (one traditional tabletop projection and the other with a \u2018tangible\u2019 aid) as spatial 3D visualization techniques, as compared to a baseline 3D rendering on a phone. We identify a significant difference in mental and physical workload between the two AR conditions in men and women. Qualitatively, through thematic coding of interviews, we identify notable observed differences ranging from device-specific physical challenges, to subdomain-specific utility within astronomy. The confluence of quantitative and qualitative results suggest a tension between workload and engagement when comparing non-AR and AR technologies. We summarize these findings and contribute them for reference in data visualization research furthering novel scientific communications in astronomy journal publications.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Author Index",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00060",
    "authors": [],
    "abstract": "N/A",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Author Index",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00072",
    "authors": [],
    "abstract": "N/A",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Bavisitter: Integrating Design Guidelines into Large Language Models for Visualization Authoring",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00032",
    "authors": [
      {
        "name": "Jiwon Choi",
        "affiliation": "Sungkyunkwan University"
      },
      {
        "name": "Jaeung Lee",
        "affiliation": "Sungkyunkwan University"
      },
      {
        "name": "Jaemin Jo",
        "affiliation": "Sungkyunkwan University"
      }
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable versatility in visualization authoring, but often generate suboptimal designs that are invalid or fail to adhere to design guidelines for effective visualization. We present Bavisitter, a natural language interface that integrates established visualization design guidelines into LLMs. Based on our survey on the design issues in LLM-generated visualizations, Bavisitter monitors the generated visualizations during a visualization authoring dialogue to detect an issue. When an issue is detected, it intervenes in the dialogue, suggesting possible solutions to the issue by modifying the prompts. We also demonstrate two use cases where Bavisitter detects and resolves design issues from the actual LLM-generated visualizations.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Bringing Data into the Conversation: Adapting Content from Business Intelligence Dashboards for Threaded Collaboration Platforms",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00024",
    "authors": [
      {
        "name": "Hyeok Kim",
        "affiliation": "Northwestern University"
      },
      {
        "name": "Arjun Srinivasan",
        "affiliation": "Tableau Research"
      },
      {
        "name": "Matthew Brehmer",
        "affiliation": "University of Waterloo"
      }
    ],
    "abstract": "To enable data-driven decision-making across organizations, data professionals need to share insights with their colleagues in context-appropriate communication channels. Many of their colleagues rely on data but are not themselves analysts; furthermore, their colleagues are reluctant or unable to use dedicated analytical applications or dashboards, and they expect communication to take place within threaded collaboration platforms such as Slack or Microsoft Teams. In this paper, we introduce a set of six strategies for adapting content from business intelligence (BI) dashboards into appropriate formats for sharing on collaboration platforms, formats that we refer to as dashboard snapshots. Informed by prior studies of enterprise communication around data, these strategies go beyond redesigning or restyling by considering varying levels of data literacy across an organization, introducing affordances for self-service question-answering, and anticipating the post-sharing lifecycle of data artifacts. These strategies involve the use of templates that are matched to common communicative intents, serving to reduce the workload of data professionals. We contribute a formal representation of these strategies and demonstrate their applicability in a comprehensive enterprise communication scenario featuring multiple stakeholders that unfolds over the span of months.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Building and Eroding: Exogenous and Endogenous Factors that Influence Subjective Trust in Visualization",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00069",
    "authors": [
      {
        "name": "R. Jordan Crouser",
        "affiliation": "N/A"
      },
      {
        "name": "Syrine Matoussi",
        "affiliation": "N/A"
      },
      {
        "name": "Lan Kung",
        "affiliation": "N/A"
      },
      {
        "name": "Saugat Pandey",
        "affiliation": "N/A"
      },
      {
        "name": "Oen G. McKinley",
        "affiliation": "N/A"
      },
      {
        "name": "Alvitta Ottley",
        "affiliation": "N/A"
      }
    ],
    "abstract": "Trust is a subjective yet fundamental component of human-computer interaction, and is a determining factor in shaping the efficacy of data visualizations. Prior research has identified five dimensions of trust assessment in visualizations (credibility, clarity, reliability, familiarity, and confidence), and observed that these dimensions tend to vary predictably along with certain features of the visualization being evaluated. This raises a further question: how do the design features driving viewers\u2019 trust assessment vary with the characteristics of the viewers themselves? By reanalyzing data from these studies through the lens of individual differences, we build a more detailed map of the relationships between design features, individual characteristics, and trust behaviors. In particular, we model the distinct contributions of endogenous design features (such as visualization type, or the use of color) and exogenous user characteristics (such as visualization literacy), as well as the interactions between them. We then use these findings to make recommendations for individualized and adaptive visualization design.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "CLEVER: A Framework for Connecting Lived Experiences with Visualisation of Electronic Records",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00034",
    "authors": [
      {
        "name": "Mai Elshehaly",
        "affiliation": "City, University of London"
      },
      {
        "name": "Lucy Eddy",
        "affiliation": "University of Bradford"
      },
      {
        "name": "Mark Mon-Williams",
        "affiliation": "University of Leeds"
      }
    ],
    "abstract": "The disconnect between insights generated from data and real-life practices of decision makers presents a number of open questions for visual analytics (VA). In public service planning, routine data are often perceived as unavailable, biased, incomplete and inconsistent across services. Decision makers often rely on qualitative data - sometimes collected through co-production - to understand the lived experience of communities before formulating a decision. We followed a subjectivist case study approach and immersed ourselves in ongoing co-production activities over the course of one year, to capture how VA can support the dialogue between population health decision-makers and the communities they serve. We present a framework for Connecting Lived Experiences with Visualisation of Electronic Records (CLEVER). The framework regards visualisation as a central component in a complex adaptive decision-making ecosystem and highlights the need to structure domain knowledge across decision contexts in Population Health Management (PHM) at clinical-, service- and district-levels. Our process for developing an initial framework comprised three steps: (i) we elicited decision-making tasks through a series of qualitative data collection activities; (ii) we developed a preliminary domain model to capture data views and a subjective view of the world through human stories; and (iii) we developed a series of visualisation prototypes to instantiate the framework and demonstrated them regularly to stakeholders. In future work, we will conduct \u2018deep dives\u2019 to systematically study the role of VA in individual stages of the framework.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Can GPT-4 Models Detect Misleading Visualizations?",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00029",
    "authors": [
      {
        "name": "Jason Alexander",
        "affiliation": "University of Massachusetts Amherst"
      },
      {
        "name": "Priyal Nanda",
        "affiliation": "University of Massachusetts Amherst"
      },
      {
        "name": "Kai-Cheng Yang",
        "affiliation": "Network Science Institute, Northeastern University"
      },
      {
        "name": "Ali Sarvghad",
        "affiliation": "University of Massachusetts Amherst"
      }
    ],
    "abstract": "The proliferation of misleading visualizations online, particularly during critical events like public health crises and elections, poses a significant risk. This study investigates the capability of GPT-4 models (4V, 4o, and 4o mini) to detect misleading visualizations. Utilizing a dataset of tweet-visualization pairs containing various visual misleaders, we test these models under four experimental conditions with different levels of guidance. We show that GPT-4 models can detect misleading visualizations with moderate accuracy without prior training (naive zero-shot) and that performance notably improves when provided with definitions of misleaders (guided zero-shot). However, a single prompt engineering technique does not yield the best results for all misleader types. Specifically, providing the models with misleader definitions and examples (guided few-shot) proves more effective for reasoning misleaders, while guided zero-shot performs better for design misleaders. This study underscores the feasibility of using large vision-language models to detect visual misinformation and the importance of prompt engineering for optimized detection accuracy.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Combining Degree of Interest Functions and Progressive Visualization",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00059",
    "authors": [
      {
        "name": "Marius Hogr\u00e4fer",
        "affiliation": "Schulz are with Aarhus University"
      },
      {
        "name": "Dominik Moritz",
        "affiliation": "Carnegie Mellon University"
      },
      {
        "name": "Adam Perer",
        "affiliation": "Carnegie Mellon University"
      },
      {
        "name": "Hans-J\u00f6rg Schulz",
        "affiliation": "Schulz are with Aarhus University"
      }
    ],
    "abstract": "When visualizing large datasets, an important goal is to emphasize data that is relevant to the task at hand. A common way of achieving this is to compute the relevance of the data using degree of interest (DOI) functions, which apply a scenario-specific metric to quantify the data items according to their relevance to the users and their tasks. These DOI values can then be used to adjust the visual encoding through mechanisms like focus+context or information hiding. For datasets too large to be visualized at once, an alternative approach is to visualize it progressively in chunks, allowing analysts to reason about partial results and concluding their analysis much earlier than had they waited for all data. Combining the advantages of both approaches to tailor the visualization seems synergistic, yet, in practice turns out to be challenging, as DOI functions require the context of all data to produce useful values, requiring lengthy computations that break analysts\u2019 flow in progressive visualization. In this paper, we propose an approach for uniting DOI functions with progressive visualization. We first introduce a new model for quantifying the user interest in analysis scenarios where the data is only partially available, by computing the interest for available data and predicting it for the rest. We then propose regression trees for implementing this approach in practice and evaluate it in benchmarks. With DOI values now available for progressive visualization as well, our approach opens the door for tailoring the visualization of large datasets to the analysis task at interactive update rates.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Compact Phase Histograms for Guided Exploration of Periodicity",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00047",
    "authors": [
      {
        "name": "Max Franke",
        "affiliation": "University of Stuttgart, Germany"
      },
      {
        "name": "Steffen Koch",
        "affiliation": "University of Stuttgart, Germany"
      }
    ],
    "abstract": "Periodically occurring accumulations of events or measured values are present in many time-dependent datasets and can be of interest for analyses. The frequency of such periodic behavior is often not known in advance, making it difficult to detect and tedious to explore. Automated analysis methods exist, but can be too costly for smooth, interactive analysis. We propose a compact visual representation that reveals periodicity by showing a phase histogram for a given period length that can be used standalone or in combination with other linked visualizations. Our approach supports guided, interactive analyses by suggesting other period lengths to explore, which are ranked based on two quality measures. We further describe how the phase can be mapped to visual representations in other views to reveal periodicity there.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Comparing Morse Complexes Using Optimal Transport: An Experimental Study",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00017",
    "authors": [
      {
        "name": "Mingzhe Li",
        "affiliation": "University of Utah"
      },
      {
        "name": "Carson Storm",
        "affiliation": "University of Utah"
      },
      {
        "name": "Austin Yang Li",
        "affiliation": "University of Utah"
      },
      {
        "name": "Tom Needham",
        "affiliation": "Florida State University"
      },
      {
        "name": "Bei Wang",
        "affiliation": "University of Utah"
      }
    ],
    "abstract": "Morse complexes and Morse-Smale complexes are topological descriptors popular in topology-based visualization. Comparing these complexes plays an important role in their applications in feature correspondences, feature tracking, symmetry detection, and uncertainty visualization. Leveraging recent advances in optimal transport, we apply a class of optimal transport distances to the comparative analysis of Morse complexes. Contrasting with existing comparative measures, such distances are easy and efficient to compute, and naturally provide structural matching between Morse complexes. We perform an experimental study involving scientific simulation datasets and discuss the effectiveness of these distances as comparative measures for Morse complexes. We also provide an initial guideline for choosing the optimal transport distances under various data assumptions.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Concept Lens: Visually Analyzing the Consistency of Semantic Manipulation in GANs",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00053",
    "authors": [
      {
        "name": "Sangwon Jeong",
        "affiliation": "Lawrence Livermore National Laboratory"
      },
      {
        "name": "Mingwei Li",
        "affiliation": "Vanderbilt University"
      },
      {
        "name": "Matthew Berger",
        "affiliation": "Vanderbilt University"
      },
      {
        "name": "Shusen Liu",
        "affiliation": "Lawrence Livermore National Laboratory"
      }
    ],
    "abstract": "As applications of generative AI become mainstream, it is important to understand what generative models are capable of producing, and the extent to which one can predictably control their outputs. In this paper, we propose a visualization design, named Concept Lens, for jointly navigating the data distribution of a generative model, and concept manipulations supported by the model. Our work is focused on modern vision-based generative adversarial networks (GAN), and their learned latent spaces, wherein concept discovery has gained significant interest as a means of image manipulation. Concept Lens is designed to support users in understanding the diversity of a provided set of concepts, the relationship between concepts, and the suitability of concepts to give semantic controls for image generation. Key to our approach is the hierarchical grouping of concepts, generated images, and the associated joint exploration. We show how Concept Lens can reveal consistent semantic manipulations for editing images, while also serving as a diagnostic tool for studying the limitations and trade-offs of concept discovery methods.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Confides: A Visual Analytics Solution for Automated Speech Recognition Analysis and Exploration",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00062",
    "authors": [
      {
        "name": "Sunwoo Ha",
        "affiliation": "N/A"
      },
      {
        "name": "Chaehun Lim",
        "affiliation": "N/A"
      },
      {
        "name": "R. Jordan Crouser",
        "affiliation": "N/A"
      },
      {
        "name": "Alvitta Ottley",
        "affiliation": "N/A"
      }
    ],
    "abstract": "Confidence scores of automatic speech recognition (ASR) outputs are often inadequately communicated, preventing its seamless integration into analytical workflows. In this paper, we introduce Confides, a visual analytic system developed in collaboration with intelligence analysts to address this issue. Confides aims to aid exploration and post-AI-transcription editing by visually representing the confidence associated with the transcription. We demonstrate how our tool can assist intelligence analysts who use ASR outputs in their analytical and exploratory tasks and how it can help mitigate misinterpretation of crucial information. We also discuss opportunities for improving textual data cleaning and model transparency for human-machine collaboration.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Connections Beyond Data: Exploring Homophily With Visualizations",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00054",
    "authors": [
      {
        "name": "Poorna Talkad Sukumar",
        "affiliation": "New York University"
      },
      {
        "name": "Maurizio Porfiri",
        "affiliation": "New York University"
      },
      {
        "name": "Oded Nov",
        "affiliation": "New York University"
      }
    ],
    "abstract": "Homophily refers to the tendency of individuals to associate with others who are similar to them in characteristics, such as, race, ethnicity, age, gender, or interests. In this paper, we investigate if individuals exhibit racial homophily when viewing visualizations, using mass shooting data in the United States as the example topic. We conducted a crowdsourced experiment (N=450) where each participant was shown a visualization displaying the counts of mass shooting victims, highlighting the counts for one of three racial groups (White, Black, or Hispanic). Participants were assigned to view visualizations highlighting their own race or a different race to assess the influence of racial concordance on changes in affect (emotion) and attitude towards gun control. While we did not find evidence of homophily, the results showed a significant negative shift in affect across all visualization conditions. Notably, political ideology significantly impacted changes in affect, with more liberal views correlating with a more negative affect change. Our findings underscore the complexity of reactions to mass shooting visualizations and suggest that future research should consider various methodological improvements to better assess homophily effects.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Copyright Page",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00003",
    "authors": [],
    "abstract": "N/A",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Copyright Page",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00003",
    "authors": [],
    "abstract": "N/A",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Counterpoint: Orchestrating Large-Scale Custom Animated Visualizations",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00011",
    "authors": [
      {
        "name": "Venkatesh Sivaraman",
        "affiliation": "Carnegie Mellon University"
      },
      {
        "name": "Frank Elavsky",
        "affiliation": "Carnegie Mellon University"
      },
      {
        "name": "Dominik Moritz",
        "affiliation": "Carnegie Mellon University"
      },
      {
        "name": "Adam Perer",
        "affiliation": "Carnegie Mellon University"
      }
    ],
    "abstract": "Custom animated visualizations of large, complex datasets are helpful across many domains, but they are hard to develop. Much of the difficulty arises from maintaining visualization state across many animated graphical elements that may change in number over time. We contribute Counterpoint, a framework for state management designed to help implement such visualizations in JavaScript. Using Counterpoint, developers can manipulate large collections of marks with reactive attributes that are easy to render in scalable APIs such as Canvas and WebGL. Counterpoint also helps orchestrate the entry and exit of graphical elements using the concept of a rendering \"stage.\" Through a performance evaluation, we show that Counterpoint adds minimal overhead over current high-performance rendering techniques while simplifying implementation. We provide two examples of visualizations created using Counterpoint that illustrate its flexibility and compatibility with other visualization toolkits as well as considerations for users with disabilities. Counterpoint is open-source and available at https://github.com/cmudig/counterpoint.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Curve Segment Neighborhood-based Vector Field Exploration",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00075",
    "authors": [
      {
        "name": "Nguyen K. Phan",
        "affiliation": "University of Houston"
      },
      {
        "name": "Guoning Chen",
        "affiliation": "University of Houston"
      }
    ],
    "abstract": "Integral curves have been widely used to represent and analyze various vector fields. In this paper, we propose a Curve Segment Neighborhood Graph (CSNG) to capture the relationships between neighboring curve segments. This graph representation enables us to adapt the fast community detection algorithm, i.e., the Louvain algorithm, to identify individual graph communities from CSNG. Our results show that these communities often correspond to the features of the flow. To achieve a multi-level interactive exploration of the detected communities, we adapt a force-directed layout that allows users to refine and re-group communities based on their domain knowledge. We incorporate the proposed techniques into an interactive system to enable effective analysis and interpretation of complex patterns in large-scale integral curve datasets.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "DATATALES: Investigating the use of Large Language Models for Authoring Data-Driven Articles",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00055",
    "authors": [
      {
        "name": "Nicole Sultanum",
        "affiliation": "Tableau Research"
      },
      {
        "name": "Arjun Srinivasan",
        "affiliation": "Tableau Research"
      }
    ],
    "abstract": "Authoring data-driven articles is a complex process requiring authors to not only analyze data for insights but also craft a cohesive narrative that effectively communicates the insights. Text generation capabilities of contemporary large language models (LLMs) present an opportunity to assist the authoring of data-driven articles and expedite the writing process. In this work, we investigate the feasibility and perceived value of leveraging LLMs to support authors of data-driven articles. We designed a prototype system, DATATALES, that leverages a LLM to generate textual narratives accompanying a given chart. Using DATATALES as a design probe, we conducted a qualitative study with 11 professionals to evaluate the concept, from which we distilled affordances and opportunities to further integrate LLMs as valuable data-driven article authoring assistants.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "DaVE - A Curated Database of Visualization Examples",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00010",
    "authors": [
      {
        "name": "Jens Koenen",
        "affiliation": "RWTH Aachen University, Germany"
      },
      {
        "name": "Marvin Petersen",
        "affiliation": "RPTU Kaiserslautern-Landau, Germany"
      },
      {
        "name": "Christoph Garth",
        "affiliation": "RPTU Kaiserslautern-Landau, Germany"
      },
      {
        "name": "Tim Gerrits",
        "affiliation": "RWTH Aachen University, Germany"
      }
    ],
    "abstract": "Visualization, from simple line plots to complex high-dimensional visual analysis systems, has established itself throughout numerous domains to explore, analyze, and evaluate data. Applying such visualizations in the context of simulation science where High-Performance Computing (HPC) produces ever-growing amounts of data that is more complex, potentially multidimensional, and multimodal, takes up resources and a high level of technological experience often not available to domain experts. In this work, we present DaVE - a curated database of visualization examples, which aims to provide state-of-the-art and advanced visualization methods that arise in the context of HPC applications. Based on domain- or data-specific descriptors entered by the user, DaVE provides a list of appropriate visualization techniques, each accompanied by descriptions, examples, references, and resources. Sample code, adaptable container templates, and recipes for easy integration in HPC applications can be downloaded for easy access to high-fidelity visualizations. While the database is currently filled with a limited number of entries based on a broad evaluation of needs and challenges of current HPC users, DaVE is designed to be easily extended by experts from both the visualization and HPC communities.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Dark Mode or Light Mode? Exploring the Impact of Contrast Polarity on Visualization Performance Between Age Groups",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00050",
    "authors": [
      {
        "name": "Zack While",
        "affiliation": "University of Massachusetts Amherst"
      },
      {
        "name": "Ali Sarvghad",
        "affiliation": "University of Massachusetts Amherst"
      }
    ],
    "abstract": "This study examines the impact of positive and negative contrast polarities (i.e., light and dark modes) on the performance of younger adults and people in their late adulthood (PLA). In a crowdsourced study with 134 participants (69 below age 60, 66 aged 60 and above), we assessed their accuracy and time performing analysis tasks across three common visualization types (Bar, Line, Scatterplot) and two contrast polarities (positive and negative). We observed that, across both age groups, the polarity that led to better performance and the resulting amount of improvement varied on an individual basis, with each polarity benefiting comparable proportions of participants. However, the contrast polarity that led to better performance did not always match their preferred polarity. Additionally, we observed that the choice of contrast polarity can have an impact on time similar to that of the choice of visualization type, resulting in an average percent difference of around 36%. These findings indicate that, overall, the effects of contrast polarity on visual analysis performance do not noticeably change with age. Furthermore, they underscore the importance of making visualizations available in both contrast polarities to better-support a broad audience with differing needs. Supplementary materials for this work can be found at https://osf.io/539a4/.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Dash: A Bimodal Data Exploration Tool for Interactive Text and Visualizations",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00059",
    "authors": [
      {
        "name": "Dennis Bromley",
        "affiliation": "Tableau Research"
      },
      {
        "name": "Vidya Setlur",
        "affiliation": "Tableau Research"
      }
    ],
    "abstract": "Integrating textual content, such as titles, annotations, and captions, with visualizations facilitates comprehension and takeaways during data exploration. Yet current tools often lack mechanisms for integrating meaningful long-form prose with visual data. This paper introduces DASH, a bimodal data exploration tool that supports integrating semantic levels into the interactive process of visualization and text-based analysis. DASH operationalizes a modified version of Lundgard et al.\u2019s semantic hierarchy model that catego-rizes data descriptions into four levels ranging from basic encodings to high-level insights. By leveraging this structured semantic level framework and a large language model\u2019s text generation capabilities, DASH enables the creation of data-driven narratives via drag-and-drop user interaction. Through a preliminary user evaluation, we discuss the utility of DASH\u2019s text and chart integration capabilities when participants perform data exploration with the tool.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Data Guards: Challenges and Solutions for Fostering Trust in Data",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00019",
    "authors": [
      {
        "name": "Nicole Sultanum",
        "affiliation": "Tableau Research, Seattle, WA, USA"
      },
      {
        "name": "Dennis Bromley",
        "affiliation": "Tableau Research, Seattle, WA, USA"
      },
      {
        "name": "Michael Correll",
        "affiliation": "Northeastern University, Portland, ME, USA"
      }
    ],
    "abstract": "From dirty data to intentional deception, there are many threats to the validity of data-driven decisions. Making use of data, especially new or unfamiliar data, therefore requires a degree of trust or verification. How is this trust established? In this paper, we present the results of a series of interviews with both producers and consumers of data artifacts (outputs of data ecosystems like spreadsheets, charts, and dashboards) aimed at understanding strategies and obstacles to building trust in data. We find a recurring need, but lack of existing standards, for data validation and verification, especially among data consumers. We therefore propose a set of data guards: methods and tools for fostering trust in data artifacts.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Data in the Wind: Evaluating Multiple-Encoding Design for Particle Motion Visualizations",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00039",
    "authors": [
      {
        "name": "Yiren Ding",
        "affiliation": "Worcester Polytechnic Institute"
      },
      {
        "name": "Lane Harrison",
        "affiliation": "Worcester Polytechnic Institute"
      }
    ],
    "abstract": "Motion is widely used in modern data visualizations, serving as a means for transitioning views and as a primary channel for conveying information. Particle flow maps have become a popular means for communicating the speed and direction of wind in engaging and informative ways. Yet there is little empirical design guidance supporting the multiple encodings these maps use, such as particle speed, particle density, and color saturation. In this paper, we investigate multiple encoding wind maps using a staircase methodology to estimate just-noticeable differences for a range of speed values across visualizations with or without motion encodings. Results suggest: 1. the multiple encodings designers use are not only aesthetically engaging\u2013 they also improve speed discriminability for the average participant. 2. The speed of particle motion should be controlled under a certain range for good information retrieval accuracy. These findings contribute empirical guidance for particle motion encoding design, and lay groundwork for future investigations as motion becomes more widely used in visualization practice.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Demystifying Spatial Dependence: Interactive Visualizations for Interpreting Local Spatial Autocorrelation",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00022",
    "authors": [
      {
        "name": "Lee Mason",
        "affiliation": "National Cancer Institute"
      },
      {
        "name": "Bl\u00e1naid Hicks",
        "affiliation": "Queen\u2019s University Belfast"
      },
      {
        "name": "Jonas Almeida",
        "affiliation": "National Cancer Institute"
      }
    ],
    "abstract": "The Local Moran\u2019s I statistic is a valuable tool for identifying localized patterns of spatial autocorrelation. Understanding these patterns is crucial in spatial analysis, but interpreting the statistic can be difficult. To simplify this process, we introduce three novel visualizations that enhance the interpretation of Local Moran\u2019s I results. These visualizations can be interactively linked to one another, and to established visualizations, to offer a more holistic exploration of the results. We provide a JavaScript library with implementations of these new visual elements, along with a web dashboard that demonstrates their integrated use.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Design Patterns in Right-to-Left Visualizations: The Case of Arabic Content",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00058",
    "authors": [
      {
        "name": "Muna Alebri",
        "affiliation": "University College, London"
      },
      {
        "name": "No\u00eblle Rakotondravony",
        "affiliation": "Worcester Polytechnic Institute"
      },
      {
        "name": "Lane Harrison",
        "affiliation": "Worcester Polytechnic Institute"
      }
    ],
    "abstract": "Data visualizations are reaching global audiences. As people who use Right-to-left (RTL) scripts constitute over a billion potential data visualization users, a need emerges to investigate how visualizations are communicated to them. Web design guidelines exist to assist designers in adapting different reading directions, yet we lack a similar standard for visualization design. This paper investigates the design patterns of visualizations with RTL scripts. We collected 128 visualizations from data-driven articles published in Arabic news outlets and analyzed their chart composition, textual elements, and sources. Our analysis suggests that designers tend to apply RTL approaches more frequently for categorical data. In other situations, we observed a mix of Left-to-right (LTR) and RTL approaches for chart directions and structures, sometimes inconsistently utilized within the same article. We reflect on this lack of clear guidelines for RTL data visualizations and derive implications for visualization authoring tools and future research directions.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Design of a Real-Time Visual Analytics Decision Support Interface to Manage Air Traffic Complexity",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00068",
    "authors": [
      {
        "name": "Elmira Zohrevandi",
        "affiliation": "Department of Science and Technology, Link\u00f6ping University, Norrk\u00f6ping, Sweden"
      },
      {
        "name": "Katerina Vrotsou",
        "affiliation": "Department of Science and Technology, Link\u00f6ping University, Norrk\u00f6ping, Sweden"
      },
      {
        "name": "Carl A. L. Westin",
        "affiliation": "Department of Science and Technology, Link\u00f6ping University, Norrk\u00f6ping, Sweden"
      },
      {
        "name": "Jonas Lundberg",
        "affiliation": "Department of Science and Technology, Link\u00f6ping University, Norrk\u00f6ping, Sweden"
      },
      {
        "name": "Anders Ynnerman",
        "affiliation": "Department of Science and Technology, Link\u00f6ping University, Norrk\u00f6ping, Sweden"
      }
    ],
    "abstract": "An essential task of an air traffic controller is to manage the traffic flow by predicting future trajectories. Complex traffic patterns are difficult to predict and manage and impose cognitive load on the air traffic controllers. In this work we present an interactive visual analytics interface which facilitates detection and resolution of complex traffic patterns for air traffic controllers. The interface supports air traffic controllers in detecting complex clusters of aircraft and further enables them to visualize and simultaneously compare how different re-routing strategies for each individual aircraft yield reduction of complexity in the entire sector for the next hour. The development of the concepts was supported by the domain-specific feedback we received from six fully licensed and operational air traffic controllers in an iterative design process over a period of 14 months.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Design of an Ecological Visual Analytics Interface for Operators of Time-Constant Processes",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00035",
    "authors": [
      {
        "name": "Elmira Zohrevandi",
        "affiliation": "Link\u00f6ping University"
      },
      {
        "name": "Emmanuel Brorsson",
        "affiliation": "S\u00f6dra Cell"
      },
      {
        "name": "Andreas Darnell",
        "affiliation": "S\u00f6dra Cell"
      },
      {
        "name": "Magnus B\u00e5ng",
        "affiliation": "Link\u00f6ping University"
      },
      {
        "name": "Jonas Lundberg",
        "affiliation": "Link\u00f6ping University"
      },
      {
        "name": "Anders Ynnerman",
        "affiliation": "Link\u00f6ping University"
      }
    ],
    "abstract": "In industrial applications where the physical parameters are highly interconnected, keeping the process flow steady is a major concern for the operators. This is caused by the sensitivity of system to the process dynamics. As a result, a slight adjustment to a control parameter can significantly affect the efficiency of the system and thus impact the financial gain. Paper pulp production is an example of such a process, where operators continuously investigate the potential of changes in the process and predict the consequences of an adjustment before making a decision. Process parameter adjustments prescribed by simulated control models cannot be fully trusted as the external disturbances and the process inherent variabilities cannot be fully incorporated into the simulations. Therefore, to assess the viability of a strategy, operators often compare the situation with the historical records and trends during which the processes in the plant ran steadily. While previous research has mostly focused on developing advanced control models to simulate complex pulp production process, this work aims to support operators analytical reasoning by provision of effective data visualization. The contributions of our design study include a domain problem characterization and a linked-view visual encoding design, which aims to enhance operator\u2019s mental models independent of particular users or scenarios. Finally, by reflecting on the advantages of our choice of task abstraction technique, inherited from the ecological interface design framework [5], we reason for the generalizability of our approach to similar industrial applications.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00027",
    "authors": [
      {
        "name": "Seongmin Lee",
        "affiliation": "Georgia Tech"
      },
      {
        "name": "Benjamin Hoover",
        "affiliation": "Georgia Tech"
      },
      {
        "name": "Hendrik Strobelt",
        "affiliation": "IBM Research"
      },
      {
        "name": "Zijie J. Wang",
        "affiliation": "Georgia Tech"
      },
      {
        "name": "ShengYun Peng",
        "affiliation": "Georgia Tech"
      },
      {
        "name": "Austin Wright",
        "affiliation": "Georgia Tech"
      },
      {
        "name": "Kevin Li",
        "affiliation": "Georgia Tech"
      },
      {
        "name": "Haekyu Park",
        "affiliation": "Georgia Tech"
      },
      {
        "name": "Haoyang Yang",
        "affiliation": "Georgia Tech"
      },
      {
        "name": "Duen Horng Polo Chau",
        "affiliation": "Georgia Tech"
      }
    ],
    "abstract": "Diffusion-based generative models\u2019 impressive ability to create convincing images has garnered global attention. However, their complex structures and operations often pose challenges for non-experts to grasp. We present Diffusion Explainer, the first interactive visualization tool that explains how Stable Diffusion transforms text prompts into images. Diffusion Explainer tightly integrates a visual overview of Stable Diffusion\u2019s complex structure with explanations of the underlying operations. By comparing image generation of prompt variants, users can discover the impact of keyword changes on image generation. A 56-participant user study demonstrates that Diffusion Explainer offers substantial learning benefits to non-experts. Our tool has been used by over 10,300 users from 124 countries at https://poloclub.github.io/diffusion-explainer/.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Do You Trust What You See? Toward A Multidimensional Measure of Trust in Visualization",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00014",
    "authors": [
      {
        "name": "Saugat Pandey",
        "affiliation": "Washington University in St. Louis"
      },
      {
        "name": "Oen G. McKinley",
        "affiliation": "Washington University in St. Louis"
      },
      {
        "name": "R. Jordan Crouser",
        "affiliation": "Smith College"
      },
      {
        "name": "Alvitta Ottley",
        "affiliation": "Washington University in St. Louis"
      }
    ],
    "abstract": "Few concepts are as ubiquitous in computational fields as trust. However, in the case of information visualization, there are several unique and complex challenges, chief among them: defining and measuring trust. In this paper, we investigate the factors that influence trust in visualizations. We draw on the literature to identify five factors likely to affect trust: credibility, clarity, reliability, familiarity, and confidence. We then conduct two studies investigating these factors\u2019 relationship with visualization design features. In the first study, participants\u2019 credibility, understanding, and reliability ratings depended on the visualization design and its source. In the second study, we find these factors also align with subjective trust rankings. Our findings suggest that these five factors are important considerations for the design of trustworthy visualizations.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Draco 2: An Extensible Platform to Model Visualization Design",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00042",
    "authors": [
      {
        "name": "Junran Yang",
        "affiliation": "University of Washington, Seattle"
      },
      {
        "name": "P\u00e9ter Ferenc Gyarmati",
        "affiliation": "University of Vienna, Vienna"
      },
      {
        "name": "Zehua Zeng",
        "affiliation": "University of Maryland, College Park"
      },
      {
        "name": "Dominik Moritz",
        "affiliation": "Carnegie Mellon University, Pittsburgh"
      }
    ],
    "abstract": "Draco introduced a constraint-based framework to model visualization design in an extensible and testable form. It provides a way to abstract design guidelines from theoretical and empirical studies and applies the knowledge in automated design tools. However, Draco is challenging to use because there is limited tooling and documentation. In response, we present Draco 2, the successor with (1) a more flexible visualization specification format, (2) a comprehensive test suite and documentation, and (3) flexible and convenient APIs. We designed Draco 2 to be more extensible and easier to integrate into visualization systems. We demonstrate these advantages and believe that they make Draco 2 a platform for future research.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Effects of data distribution and granularity on color semantics for colormap data visualizations",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00011",
    "authors": [
      {
        "name": "Clementine Zimnicki",
        "affiliation": "University of Wisconsin\u2013Madison"
      },
      {
        "name": "Chin Tseng",
        "affiliation": "University of North Carolina, Chapel Hill"
      },
      {
        "name": "Danielle Albers Szafir",
        "affiliation": "UNC, Chapel Hill"
      },
      {
        "name": "Karen B Schloss",
        "affiliation": "UW\u2013Madison"
      }
    ],
    "abstract": "To create effective data visualizations, it helps to represent data using visual features in intuitive ways. When visualization designs match observer expectations, visualizations are easier to interpret. Prior work suggests that several factors influence such expectations. For example, the dark-is-more bias leads observers to infer that darker colors map to larger quantities, and the opaque-is-more bias leads them to infer that regions appearing more opaque (given the background color) map to larger quantities. Previous work suggested that the background color only plays a role if visualizations appear to vary in opacity. The present study challenges this claim. We hypothesized that the background color would modulate inferred mappings for colormaps that should not appear to vary in opacity (by previous measures) if the visualization appeared to have a \u201chole\u201d that revealed the background behind the map (hole hypothesis). We found that spatial aspects of the map contributed to inferred mappings, though the effects were inconsistent with the hole hypothesis. Our work raises new questions about how spatial distributions of data influence color semantics in colormap data visualizations.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Enabling Multimodal User Interactions for Genomics Visualization Creation",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00031",
    "authors": [
      {
        "name": "Qianwen Wang",
        "affiliation": "Harvard Medical School, Boston, MA, USA"
      },
      {
        "name": "Xiao Liu",
        "affiliation": "Harvard Medical School, Boston, MA, USA"
      },
      {
        "name": "Man Qing Liang",
        "affiliation": "Harvard Medical School, Boston, MA, USA"
      },
      {
        "name": "Sehi L\u2019Yi",
        "affiliation": "Harvard Medical School, Boston, MA, USA"
      },
      {
        "name": "Nils Gehlenborg",
        "affiliation": "Harvard Medical School, Boston, MA, USA"
      }
    ],
    "abstract": "Visualization plays an important role in extracting insights from complex and large-scale genomics data. Traditional graphical user interfaces (GUIs) offer limited flexibility for custom visualizations. Our prior work, Gosling, enables expressive visualization creation using a grammar-based approach, but beginners may face challenges in constructing complex visualizations. To address this, we explore multimodal interactions, including sketches, example images, and natural language inputs, to streamline visualization creation. Specifically, we customize two deep learning models (YOLO v7 and GPT3.5) to interpret user interactions and convert them into Gosling specifications. A workflow is proposed to progressively introduce and integrate multimodal interactions. We then present use cases demonstrating their effectiveness and identify challenges and opportunities for future research.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Evaluating the Semantic Profiling Abilities of LLMs for Natural Language Utterances in Data Visualization",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00060",
    "authors": [
      {
        "name": "Hannah K. Bako",
        "affiliation": "University of Maryland"
      },
      {
        "name": "Arshnoor Bhutani",
        "affiliation": "University of Maryland"
      },
      {
        "name": "Xinyi Liu",
        "affiliation": "University of Texas at Austin"
      },
      {
        "name": "Kwesi A. Cobbina",
        "affiliation": "University of Maryland"
      },
      {
        "name": "Zhicheng Liu",
        "affiliation": "University of Mayland"
      }
    ],
    "abstract": "Automatically generating data visualizations in response to human utterances on datasets necessitates a deep semantic understanding of the utterance, including implicit and explicit references to data attributes, visualization tasks, and necessary data preparation steps. Natural Language Interfaces (NLIs) for data visualization have explored ways to infer such information, yet challenges persist due to inherent uncertainty in human speech. Recent advances in Large Language Models (LLMs) provide an avenue to address these challenges, but their ability to extract the relevant semantic information remains unexplored. In this study, we evaluate four publicly available LLMs (GPT-4, Gemini-Pro, Llama3, and Mixtral), investigating their ability to comprehend utterances even in the presence of uncertainty and identify the relevant data context and visual tasks. Our findings reveal that LLMs are sensitive to uncertainties in utterances. Despite this sensitivity, they are able to extract the relevant data context. However, LLMs struggle with inferring visualization tasks. Based on these results, we highlight future research directions on using LLMs for visualization generation. Our supplementary materials have been shared on GitHub: https://github.com/hdi-umd/Semantic_Profiling_LLM_Evaluation.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Evaluation of cinematic volume rendering open-source and commercial solutions for the exploration of congenital heart data",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00024",
    "authors": [
      {
        "name": "Irum Baseer",
        "affiliation": "Universitat Pompeu Fabra"
      },
      {
        "name": "Israel Valverde",
        "affiliation": "Institute of Biomedicine of Seville"
      },
      {
        "name": "Abdel H. Moustafa",
        "affiliation": "Hospital de la Santa Creu i Sant Pau"
      },
      {
        "name": "Josep Blat",
        "affiliation": "Universitat Pompeu Fabra"
      },
      {
        "name": "Oscar Camara",
        "affiliation": "Universitat Pompeu Fabra"
      }
    ],
    "abstract": "Detailed anatomical information is essential to optimize medical decisions for surgical and pre-operative planning in patients with congenital heart disease. The visualization techniques commonly used in clinical routine for the exploration of complex cardiac data are based on multi-planar reformations, maximum intensity projection, and volume rendering, which rely on basic lighting models prone to image distortion. On the other hand, cinematic rendering (CR), a three-dimensional visualization technique based on physically-based rendering methods, can create volumetric images with high fidelity. However, there are a lot of parameters involved in CR that affect the visualization results, thus being dependent on the user\u2019s experience and requiring detailed evaluation protocols to compare available solutions. In this study, we have analyzed the impact of the most relevant parameters in a CR pipeline developed in the open-source version of the MeVisLab framework for the visualization of the heart anatomy of three congenital patients and two adults from CT images. The resulting visualizations were compared to a commercial tool used in the clinics with a questionnaire filled in by clinical users, providing similar definitions of structures, depth perception, texture appearance, realism, and diagnostic ability.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "ExoplanetExplorer: Contextual Visualization of Exoplanet Systems",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00025",
    "authors": [
      {
        "name": "Emma Broman",
        "affiliation": "Link\u00f6ping University"
      },
      {
        "name": "Jacqueline Faherty",
        "affiliation": "American Museum of Natural History"
      },
      {
        "name": "Laura Kreidberg",
        "affiliation": "Max Planck Institute for Astronomy"
      },
      {
        "name": "Sebastian Zieba",
        "affiliation": "Max Planck Institute for Astronomy"
      },
      {
        "name": "Charles Hansen",
        "affiliation": "University of Utah, Link\u00f6ping University"
      },
      {
        "name": "Anders Ynnerman",
        "affiliation": "Link\u00f6ping University"
      },
      {
        "name": "Alexander Bock",
        "affiliation": "Link\u00f6ping University"
      }
    ],
    "abstract": "An exoplanet is a planet outside of our solar system. Researchers study known exoplanets and gather data about them through observations and derived data. Ongoing efforts involve finding planets with an environment that supports life, which likely exists in what is known as the habitable zone around a star. Through a participatory design process, we developed a tool that enables the exploration of exoplanet attribute data and provides contextual visual information in a 3D spatial view that seamlessly presents an overview and a system view showing particular exoplanet systems.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Explain-and-Test: An Interactive Machine Learning Framework for Exploring Text Embeddings",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00052",
    "authors": [
      {
        "name": "Shivam Raval",
        "affiliation": "Harvard University"
      },
      {
        "name": "Carolyn Wang",
        "affiliation": "Harvard College"
      },
      {
        "name": "Fernanda Vi\u00e9gas",
        "affiliation": "Harvard University, Google Research"
      },
      {
        "name": "Martin Wattenberg",
        "affiliation": "Harvard University, Google Research"
      }
    ],
    "abstract": "Text embeddings\u2013mappings of collections of text to points in high-dimensional space\u2013are a common object of analysis. A classic method to visualize these embeddings is to create a nonlinear projection to two dimensions and look for clusters and other structures in the resulting map. Explaining why certain texts cluster together, however, can be difficult. In this paper, we introduce a human-in-the-loop framework for applying machine learning (ML) to this challenge. The framework has two stages: (1) explain, in which we use ML to produce a description of a pattern; and (2) test, in which the user can verify the explanation by entering new text that fits the pattern, and sees where it appears on the map. If the new text is mapped to the original cluster, that is evidence in favor of the ML-generated explanation. We illustrate this process with a visualization application that provides two kinds of explanations: Natural Language Explanations and Contrastive PhraseClouds. Scenarios on exploring academic papers and literary work showcase the benefit of our workflow in discovering related topics and analyzing thematic differences in text.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Exploring the Capability of LLMs in Performing Low-Level Visual Analytic Tasks on SVG Data Visualizations",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00033",
    "authors": [
      {
        "name": "Zhongzheng Xu",
        "affiliation": "Brown University"
      },
      {
        "name": "Emily Wall",
        "affiliation": "Emory University"
      }
    ],
    "abstract": "Data visualizations help extract insights from datasets, but reaching these insights requires decomposing high level goals into low-level analytic tasks that can be complex due to varying degrees of data literacy and visualization experience. Recent advancements in large language models (LLMs) have shown promise for lowering barriers for users to achieve tasks such as writing code and may likewise facilitate visualization insight. Scalable Vector Graphics (SVG), a text-based image format common in data visualizations, matches well with the text sequence processing of transformer-based LLMs. In this paper, we explore the capability of LLMs to perform 10 low-level visual analytic tasks defined by Amar, Eagan, and Stasko directly on SVG-based visualizations [2]. Using zero-shot prompts, we instruct the models to provide responses or modify the SVG code based on given visualizations. Our findings demonstrate that LLMs can effectively modify existing SVG visualizations for some tasks like Cluster but perform poorly on tasks requiring mathematical operations like Compute Derived Value. We also discovered that LLM performance can vary based on factors such as the number of data points, the presence of value labels, and the chart type. Our findings contribute to gauging the general capabilities of LLMs and highlight the need for further exploration and development to fully harness their potential in supporting visual analytic tasks.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "FAVis: Visual Analytics of Factor Analysis for Psychological Research",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00018",
    "authors": [
      {
        "name": "Yikai Lu",
        "affiliation": "University of Notre Dame"
      },
      {
        "name": "Chaoli Wang",
        "affiliation": "University of Notre Dame"
      }
    ],
    "abstract": "Psychological research often involves understanding psychological constructs through conducting factor analysis on data collected by a questionnaire, which can comprise hundreds of questions. Without interactive systems for interpreting factor models, researchers are frequently exposed to subjectivity, potentially leading to misinterpretations or overlooked crucial information. This paper introduces FAVis, a novel interactive visualization tool designed to aid researchers in interpreting and evaluating factor analysis results. FAVis enhances the understanding of relationships between variables and factors by supporting multiple views for visualizing factor loadings and correlations, allowing users to analyze information from various perspectives. The primary feature of FAVis is to enable users to set optimal thresholds for factor loadings to balance clarity and information retention. FAVis also allows users to assign tags to variables, enhancing the understanding of factors by linking them to their associated psychological constructs. Our user study demonstrates the utility of FAVis in various tasks.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "FCNR: Fast Compressive Neural Representation of Visualization Images",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00014",
    "authors": [
      {
        "name": "Yunfei Lu",
        "affiliation": "University of Notre Dame"
      },
      {
        "name": "Pengfei Gu",
        "affiliation": "University of Notre Dame"
      },
      {
        "name": "Chaoli Wang",
        "affiliation": "University of Notre Dame"
      }
    ],
    "abstract": "We present FCNR, a fast compressive neural representation for tens of thousands of visualization images under varying viewpoints and timesteps. The existing NeRVI solution, albeit enjoying a high compression ratio, incurs slow speeds in encoding and decoding. Built on the recent advances in stereo image compression, FCNR assimilates stereo context modules and joint context transfer modules to compress image pairs. Our solution significantly improves encoding and decoding speed while maintaining high reconstruction quality and satisfying compression ratio. To demonstrate its effectiveness, we compare FCNR with state-of-the-art neural compression methods, including E-NeRV, HNeRV, NeRVI, and ECSIC. The source code can be found at https://github.com/YunfeiLu0112/FCNR.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Fast Fiber Line Extraction for 2D Bivariate Scalar Fields",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00021",
    "authors": [
      {
        "name": "Felix Raith",
        "affiliation": "Leipzig University"
      },
      {
        "name": "Baldwin Nsonga",
        "affiliation": "Leipzig University"
      },
      {
        "name": "Gerik Scheuermann",
        "affiliation": "Leipzig University"
      },
      {
        "name": "Christian Heine",
        "affiliation": "Leipzig University"
      }
    ],
    "abstract": "Extracting level sets from scalar data is a fundamental operation in visualization with many applications. Recently, the concept of level set extraction has been extended to bivariate scalar fields. Prior work on vector field equivalence, wherein an analyst marks a region in the domain and is shown other regions in the domain with similar vector values, pointed out the need to make this extraction operation fast, so that analysts can work interactively. To date, the fast extraction of level sets from bivariate scalar fields has not been researched as extensively as for the univariate case. In this paper, we present a novel algorithm that extracts fiber lines, i.e., the preimages of so called control polygons (FSCP), for bivariate 2D data by joint traversal of bounding volume hierarchies for both grid and FSCP elements. We performed an extensive evaluation, comparing our method to a two-dimensional adaptation of the method proposed by Klacansky et al., as well as to the naive approach for fiber line extraction. The evaluation incorporates a vast array of configurations in several datasets. We found that our method provides a speedup of several orders of magnitudes compared to the naive algorithm and requires two thirds of the computation time compared to Klacansky et al. adapted for 2D.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Feature Clock: High-Dimensional Effects in Two-Dimensional Plots",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00038",
    "authors": [
      {
        "name": "Olga Ovcharenko",
        "affiliation": "ETH Z\u00fcrich"
      },
      {
        "name": "Rita Sevastjanova",
        "affiliation": "ETH Z\u00fcrich"
      },
      {
        "name": "Valentina Boeva",
        "affiliation": "ETH Z\u00fcrich"
      }
    ],
    "abstract": "Humans struggle to perceive and interpret high-dimensional data. Therefore, high-dimensional data are often projected into two dimensions for visualization. Many applications benefit from complex nonlinear dimensionality reduction techniques, but the effects of individual high-dimensional features are hard to explain in the two-dimensional space. Most visualization solutions use multiple two-dimensional plots, each showing the effect of one high-dimensional feature in two dimensions; this approach creates a need for a visual inspection of k plots for a k-dimensional input space. Our solution, Feature Clock, provides a novel approach that reduces the need to inspect these k plots to grasp the influence of original features on the data structure depicted in two dimensions. Feature Clock enhances the explainability and compactness of visualizations of embedded data and is available in an open-source Python library1.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Fields, Bridges, and Foundations: How Researchers Browse Citation Network Visualizations",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00037",
    "authors": [
      {
        "name": "Kiroong Choe",
        "affiliation": "Seoul National University"
      },
      {
        "name": "Eunhye Kim",
        "affiliation": "Seoul National University"
      },
      {
        "name": "Sangwon Park",
        "affiliation": "Seoul National University"
      },
      {
        "name": "Jinwook Seo",
        "affiliation": "Seoul National University"
      }
    ],
    "abstract": "Visualizing citation relations with network structures is widely used, but the visual complexity can make it challenging for individual researchers trying to navigate them. We collected data from 18 researchers with an interface that we designed using network simplification methods and analyzed how users browsed and identified important papers. Our analysis reveals six major patterns used for identifying papers of interest, which can be categorized into three key components: Fields, Bridges, and Foundations, each viewed from two distinct perspectives: layout-oriented and connection-oriented. The connection-oriented approach was found to be more reliable for selecting relevant papers, but the layout-oriented method was adopted more often, even though it led to unexpected results and user frustration. Our findings emphasize the importance of integrating these components and the necessity to balance visual layouts with meaningful connections to enhance the effectiveness of citation networks in academic browsing systems.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "From Graphs to Words: A Computer-Assisted Framework for the Production of Accessible Text Descriptions",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00067",
    "authors": [
      {
        "name": "Qiang Xu",
        "affiliation": "Polytechnique Montr\u00e9al"
      },
      {
        "name": "Thomas Hurtut",
        "affiliation": "Polytechnique Montr\u00e9al"
      }
    ],
    "abstract": "In the digital landscape, the ubiquity of data visualizations in media underscores the necessity for accessibility to ensure inclusivity for all users, including those with visual impairments. Current visual content often fails to cater to the needs of screen reader users due to the absence of comprehensive textual descriptions. To address this gap, we propose in this paper a framework designed to empower media content creators to transform charts into descriptive narratives. This tool not only facilitates the understanding of complex visual data through text but also fosters a broader awareness of accessibility in digital content creation. Through the application of this framework, users can interpret and convey the insights of data visualizations more effectively, accommodating a diverse audience. Our evaluations reveal that this tool not only enhances the comprehension of data visualizations but also promotes new perspectives on the represented data, thereby broadening the interpretative possibilities for all users.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "GeneticFlow: Exploring Scholar Impact with Interactive Visualization",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00022",
    "authors": [
      {
        "name": "Fengli Xiao",
        "affiliation": "Beihang University, Beijing, China"
      },
      {
        "name": "Lei Shi",
        "affiliation": "Beihang University, Beijing, China"
      }
    ],
    "abstract": "Visualizing a scholar\u2019s scientific impact is important for many challenging tasks in academia such as tenure evaluation and award selection. Existing visualization and profiling approaches do not focus on the analysis of individual scholar\u2019s impact, or they are too abstract to provide detailed interpretation of high-impact scholars. This work builds over a new scholar-centric impact-oriented profiling method called GeneticFlow. We propose a visualization design of scholar\u2019s self-citation graphs using a time-dependent, hierarchical representation method. The graph visualization is augmented with color-coded topic information trained with cutting-edge deep learning techniques, and also temporal trend chart to illustrate the dynamics of topic/impact evolution. The visualization method is validated on a benchmark dataset established for the visualization field. Visualization results reveal key patterns of high-impact scholars and also demonstrate its capability to serve ordinary researchers for their impact visualization task.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "GhostUMAP: Measuring Pointwise Instability in Dimensionality Reduction",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00040",
    "authors": [
      {
        "name": "Myeongwon Jung",
        "affiliation": "Sungkyunkwan University"
      },
      {
        "name": "Takanori Fujiwara",
        "affiliation": "Link\u00f6ping University"
      },
      {
        "name": "Jaemin Jo",
        "affiliation": "Sungkyunkwan University"
      }
    ],
    "abstract": "Although many dimensionality reduction (DR) techniques employ stochastic methods for computational efficiency, such as negative sampling or stochastic gradient descent, their impact on the projection has been underexplored. In this work, we investigate how such stochasticity affects the stability of projections and present a novel DR technique, GhostUMAP, to measure the pointwise instability of projections. Our idea is to introduce clones of data points, \"ghosts\", into UMAP\u2019s layout optimization process. Ghosts are designed to be completely passive: they do not affect any others but are influenced by attractive and repulsive forces from the original data points. After a single optimization run, GhostUMAP can capture the projection instability of data points by measuring the variance with the projected positions of their ghosts. We also present a successive halving technique to reduce the computation of GhostUMAP. Our results suggest that Ghost-UMAP can reveal unstable data points with a reasonable computational overhead.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Gridded Glyphmaps for Supporting Spatial COVID-19 Modelling",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00009",
    "authors": [
      {
        "name": "Aidan Slingsby",
        "affiliation": "Department of Computer Science City, University of London"
      },
      {
        "name": "Richard Reeve",
        "affiliation": "School of Biodiversity, One Health & Veterinary Medicine, University of Glasgow"
      },
      {
        "name": "Claire Harris",
        "affiliation": "Biomathematics and Statistics Scotland, James Hutton Institute"
      }
    ],
    "abstract": "We describe our use of gridded glyphmaps to support development of a repurposed COVID-19 infection model during the height of the pandemic. We found that gridded glyphmaps\u2019 ability to interactive summarise multivariate model input, intermediate results and outputs across multiple scales supported our model development tasks in ways that the modellers had not previously seen. We recount our experiences, reflect on potential to support more spatial model development more generally and suggest areas of further work.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Gridlines Mitigate Sine Illusion in Line Charts",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00057",
    "authors": [
      {
        "name": "Clayton Knittel",
        "affiliation": "Google"
      },
      {
        "name": "Jane Awuah",
        "affiliation": "Georgia Institute of Technology"
      },
      {
        "name": "Steven Franconeri",
        "affiliation": "Northwestern University"
      },
      {
        "name": "Cindy Xiong Bearfield",
        "affiliation": "Georgia Institute of Technology"
      }
    ],
    "abstract": "The sine illusion is an underestimation of the difference between two lines when both lines have increasing slopes. We evaluate three visual manipulations on mitigating sine illusions: dotted lines, aligned gridlines, and offset gridlines via a user study. We asked participants to compare the deltas between two lines at two time points and found aligned gridlines to be the most effective in mitigating sine illusions. Using data from the user study, we produced a model that predicts the impact of the sine illusion in line charts by accounting for the ratio of the vertical distance between the two points of comparison. When the ratio is less than 50%, participants begin to be influenced by the sine illusion. This effect can be significantly exacerbated when the difference between the two deltas falls under 30%. We compared two explanations for the sine illusion based on our data: either participants were mistakenly using the perpendicular distance between the two lines to make their comparison (the perpendicular explanation), or they incorrectly relied on the length of the line segment perpendicular to the angle bisector of the bottom and top lines (the equal triangle explanation). We found the equal triangle explanation to be the more predictive model explaining participant behaviors.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Groot: A System for Editing and Configuring Automated Data Insights",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00015",
    "authors": [
      {
        "name": "Sneha Gathani",
        "affiliation": "University of Maryland, College Park"
      },
      {
        "name": "Anamaria Crisan",
        "affiliation": "Tableau Research"
      },
      {
        "name": "Vidya Setlur",
        "affiliation": "Tableau Research"
      },
      {
        "name": "Arjun Srinivasan",
        "affiliation": "Tableau Research"
      }
    ],
    "abstract": "Visualization tools now commonly present automated insights highlighting salient data patterns, including correlations, distributions, outliers, and differences, among others. While these insights are valuable for data exploration and chart interpretation, users currently only have a binary choice of accepting or rejecting them, lacking the flexibility to refine the system logic or customize the insight generation process. To address this limitation, we present Groot, a prototype system that allows users to proactively specify and refine automated data insights. The system allows users to directly manipulate chart elements to receive insight recommendations based on their selections. Additionally, Groot provides users with a manual editing interface to customize, reconfigure, or add new insights to individual charts and propagate them to future explorations. We describe a usage scenario to illustrate how these features collectively support insight editing and configuration and discuss opportunities for future work, including incorporating Large Language Models (LLMs), improving semantic data and visualization search, and supporting insight management.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Guided Statistical Workflows with Interactive Explanations and Assumption Checking",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00013",
    "authors": [
      {
        "name": "Yuqi Zhang",
        "affiliation": "New York University"
      },
      {
        "name": "Adam Perer",
        "affiliation": "Carnegie Mellon University"
      },
      {
        "name": "Will Epperson",
        "affiliation": "Carnegie Mellon University"
      }
    ],
    "abstract": "Statistical practices such as building regression models or running hypothesis tests rely on following rigorous procedures of steps and verifying assumptions on data to produce valid results. However, common statistical tools do not verify users\u2019 decision choices and provide low-level statistical functions without instructions on the whole analysis practice. Users can easily misuse analysis methods, potentially decreasing the validity of results. To address this problem, we introduce GuidedStats, an interactive interface within computational notebooks that encapsulates guidance, models, visualization, and exportable results into interactive workflows. It breaks down typical analysis processes, such as linear regression and two-sample T-tests, into interactive steps supplemented with automatic visualizations and explanations for step-wise evaluation. Users can iterate on input choices to refine their models, while recommended actions and exports allow the user to continue their analysis in code. Case studies show how GuidedStats offers valuable instructions for conducting fluid statistical analyses while finding possible assumption violations in the underlying data, supporting flexible and accurate statistical analyses.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "HAiVA: Hybrid AI-assisted Visual Analysis Framework to Study the Effects of Cloud Properties on Climate Patterns",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00054",
    "authors": [
      {
        "name": "Subhashis Hazarika",
        "affiliation": "Palo Alto Research Center"
      },
      {
        "name": "Haruki Hirasawa",
        "affiliation": "University of Victoria"
      },
      {
        "name": "Sookyung Kim",
        "affiliation": "Palo Alto Research Center"
      },
      {
        "name": "Kalai Ramea",
        "affiliation": "Palo Alto Research Center"
      },
      {
        "name": "Salva R. Cachay",
        "affiliation": "University of California, San Dieago"
      },
      {
        "name": "Peetak Mitra",
        "affiliation": "Excarta"
      },
      {
        "name": "Dipti Hingmire",
        "affiliation": "University of Victoria"
      },
      {
        "name": "Hansi Singh",
        "affiliation": "University of Victoria"
      },
      {
        "name": "Phil J. Rasch",
        "affiliation": "University of Washington"
      }
    ],
    "abstract": "Clouds have a significant impact on the Earth\u2019s climate system. They play a vital role in modulating Earth\u2019s radiation budget and driving regional changes in temperature and precipitation. This makes clouds ideal for climate intervention techniques like Marine Cloud Brightening (MCB) which refers to modification in cloud reflectivity, thereby cooling the surrounding region. However, to avoid unintended effects of MCB, we need a better understanding of the complex cloud to climate response function. Designing and testing such interventions scenarios with conventional Earth System Models is computationally expensive. Therefore, we propose a hybrid AI-assisted visual analysis framework to drive such scientific studies and facilitate interactive \u201cwhat-if\u201d investigation of different MCB intervention scenarios to assess their intended and unintended impacts on climate patterns. We work with a team of climate scientists to develop a suite of hybrid AI models emulating cloud-climate response function and design a tightly coupled frontend interactive visual analysis system to perform different MCB intervention experiments.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "How \u201cApplied\u201d is Fifteen Years of VAST conference?",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00033",
    "authors": [
      {
        "name": "Lei Shi",
        "affiliation": "Beihang University, Beijing, China"
      },
      {
        "name": "Lei Xia",
        "affiliation": "Beihang University, Beijing, China"
      },
      {
        "name": "Zipeng Liu",
        "affiliation": "Beihang University, Beijing, China"
      },
      {
        "name": "Ye Sun",
        "affiliation": "Beihang University, Beijing, China"
      },
      {
        "name": "Huijie Guo",
        "affiliation": "Beihang University, Beijing, China"
      },
      {
        "name": "Klaus Mueller",
        "affiliation": "Stony Brook University, New York, United States"
      }
    ],
    "abstract": "Visual analytics (VA) science and technology emerge as a promising methodology in visualization and data science in the new century. Application-driven research continues to contribute significantly to the development of VA, as well as in a broader scope of VIS. However, existing studies on the trend and impact of VA/VIS application research stay at a commentary and subjective level, using methods such as panel discussions and expert interviews. On the contrary, this work presents a first study on VA application research using data-driven methodology with cutting-edge machine learning algorithms, achieving both objective and scalable goals. Experiment results demonstrate the validity of our method with high F1 scores up to 0.89 for the inference of VA application papers on both the expert-labeled benchmark dataset and two external validation data sources. Inference results on 15 years of VAST conference papers also narrate interesting patterns in VA application research\u2019s origin, trend, and constitution.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Hypertrix: An indicatrix for high-dimensional visualizations",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00073",
    "authors": [
      {
        "name": "Shivam Raval",
        "affiliation": "Harvard University"
      },
      {
        "name": "Fernanda Vi\u00e9gas",
        "affiliation": "Harvard University Google Research"
      },
      {
        "name": "Martin Wattenberg",
        "affiliation": "Harvard University Google Research"
      }
    ],
    "abstract": "Visualizing high dimensional data is challenging, since any dimensionality reduction technique will distort distances. A classic method in cartography\u2013Tissot\u2019s Indicatrix, specific to sphere-to-plane maps\u2013 visualizes distortion using ellipses. Inspired by this idea, we describe the hypertrix: a method for representing distortions that occur when data is projected from arbitrarily high dimensions onto a 2D plane. We demonstrate our technique through synthetic and real-world datasets, and describe how this indicatrix can guide interpretations of nonlinear dimensionality reduction.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "ImageSI: Semantic Interaction for Deep Learning Image Projections",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00026",
    "authors": [
      {
        "name": "Jiayue Lin",
        "affiliation": "Virginia Tech"
      },
      {
        "name": "Rebecca Faust",
        "affiliation": "Tulane University"
      },
      {
        "name": "Chris North",
        "affiliation": "Virginia Tech"
      }
    ],
    "abstract": "Semantic interaction (SI) in Dimension Reduction (DR) of images allows users to incorporate feedback through direct manipulation of the 2D positions of images. Through interaction, users specify a set of pairwise relationships that the DR should aim to capture. Existing methods for images incorporate feedback into the DR through feature weights on abstract embedding features. However, if the original embedding features do not suitably capture the users\u2019 task then the DR cannot either. We propose ImageSI, an SI method for image DR that incorporates user feedback directly into the image model to update the underlying embeddings, rather than weighting them. In doing so, ImageSI ensures that the embeddings suitably capture the features necessary for the task so that the DR can subsequently organize images using those features. We present two variations of ImageSI using different loss functions - ${\\text{ImageS}}{{\\text{I}}_{{\\text{MD}}{{\\text{S}}^{ - 1}}}}$, which prioritizes the explicit pairwise relationships from the interaction and ImageSITriplet, which prioritizes clustering, using the interaction to define groups of images. Finally, we present a usage scenario and a simulation-based evaluation to demonstrate the utility of ImageSI and compare it to current methods.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Improving Property Graph Layouts by Leveraging Attribute Similarity for Structurally Equivalent Nodes",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00036",
    "authors": [
      {
        "name": "Patrick Mackey",
        "affiliation": "Pacific Northwest National Laboratory"
      },
      {
        "name": "Jacob Miller",
        "affiliation": "Pacific Northwest National Laboratory"
      },
      {
        "name": "Liz Faultersack",
        "affiliation": "Pacific Northwest National Laboratory"
      }
    ],
    "abstract": "Many real-world networks contain structurally-equivalent nodes. These are defined as vertices that share the same set of neighboring nodes, making them interchangeable with a traditional graph layout approach. However, many real-world graphs also have properties associated with nodes, adding additional meaning to them. We present an approach for swapping locations of structurally-equivalent nodes in graph layout so that those with more similar properties have closer proximity to each other. This improves the usefulness of the visualization from an attribute perspective without negatively impacting the visualization from a structural perspective. We include an algorithm for finding these sets of nodes in linear time, as well as methodologies for ordering nodes based on their attribute similarity, which works for scalar, ordinal, multidimensional, and categorical data.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Indy Survey Tool: A Framework to Unearth Correlations in Survey Data",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00038",
    "authors": [
      {
        "name": "Tarik Crnovrsanin",
        "affiliation": "Northeastern University"
      },
      {
        "name": "Sara Di Bartolomeo",
        "affiliation": "Northeastern University"
      },
      {
        "name": "Connor Wilson",
        "affiliation": "Northeastern University"
      },
      {
        "name": "Cody Dunne",
        "affiliation": "Northeastern University"
      }
    ],
    "abstract": "Survey companion websites allow users to explore collected survey information more deeply, as well as update or add entries for papers. These sites can help information stay relevant past the original release date of the survey paper. However, creating and maintaining a website can be laborious and difficult, especially when authors might not be experienced with programming. We introduce Indy Survey Tool to help authors develop companion websites for survey papers across diverse fields of study. The tool\u2019s core aim is to identify correlations between categorizations of papers. To accomplish this, the tool offers multiple combined filters and correlation matrix visualizations that enable users to explore the data from diverse perspectives. The tool\u2019s visualizations, list of papers, and filters are harmoniously integrated and highly responsive, providing users with feedback based on their selections. Identifying correlations in survey papers is a pivotal aspect of research, as it can enable the recognition of common combinations of categorizations within the papers\u2014as well as highlight any omissions. The versatility of Indy Survey Tool enables researchers to delve into the correlations between categorizations in survey data, an essential aspect of research that can reveal gaps in the literature and highlight promising areas for future exploration. A preprint and supplemental material for the paper can be found at osf.io/tdhqn.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Integrating Annotations for Sonifications and Physicalizations",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00063",
    "authors": [
      {
        "name": "Rhys Sorenson-Graff",
        "affiliation": "Whitman College"
      },
      {
        "name": "S. Sandra Bae",
        "affiliation": "CU Boulder"
      },
      {
        "name": "Jordan Wirfs-Brock",
        "affiliation": "Whitman College"
      }
    ],
    "abstract": "Annotations are a critical component of visualizations, helping viewers interpret the visual representation and highlighting critical data insights. Despite their significant role, we lack an understanding of how annotations can be incorporated into other data representations, such as physicalizations and sonifications. Given the emergent nature of these representations, sonifications, and physicalizations lack formalized conventions (e.g., design space, vocabulary) that can introduce challenges for audiences to interpret the intended data encoding. To address this challenge, this work focuses on how annotations can be more tightly integrated into the design process of creating sonifications and physicalizations. In an exploratory study with 13 designers, we explore how visualization annotation techniques can be adapted to sonic and physical modalities. Our work highlights how annotations for sonification and physicalizations are inseparable from their data encodings.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Intuitive Design of Deep Learning Models through Visual Feedback",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00030",
    "authors": [
      {
        "name": "JunYoung Choi",
        "affiliation": "VIENCE Inc., Korea University"
      },
      {
        "name": "Sohee Park",
        "affiliation": "VIENCE Inc."
      },
      {
        "name": "GaYeon Koh",
        "affiliation": "Korea University"
      },
      {
        "name": "Youngseo Kim",
        "affiliation": "VIENCE Inc."
      },
      {
        "name": "Won-Ki Jeong",
        "affiliation": "VIENCE Inc., Korea University"
      }
    ],
    "abstract": "In the rapidly evolving field of deep learning, traditional methodologies for designing models predominantly rely on code-based frameworks. While these approaches provide flexibility, they create a significant barrier to entry for non-experts and obscure the immediate impact of architectural decisions on model performance. In response to this challenge, recent no-code approaches have been developed with the aim of enabling easy model development through graphical interfaces. However, both traditional and no-code methodologies share a common limitation that the inability to predict model outcomes or identify issues without executing the model. To address this limitation, we introduce an intuitive visual feedback-based no-code approach to visualize and analyze deep learning models during the design phase. This approach utilizes dataflow-based visual programming with dynamic visual encoding of model architecture. A user study was conducted with deep learning developers to demonstrate the effectiveness of our approach in enhancing the model design process, improving model understanding, and facilitating a more intuitive development experience. The findings of this study suggest that real-time architectural visualization significantly contributes to more efficient model development and a deeper understanding of model behaviors.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Investigating the Apple Vision Pro Spatial Computing Platform for GPU-Based Volume Visualization",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00044",
    "authors": [
      {
        "name": "Camilla Hrycak",
        "affiliation": "University of Duisburg-Essen"
      },
      {
        "name": "David Lewakis",
        "affiliation": "University of Duisburg-Essen"
      },
      {
        "name": "Jens Kr\u00fcger",
        "affiliation": "University of Duisburg-Essen"
      }
    ],
    "abstract": "In this paper, we analyze the Apple Vision Pro hardware and the visionOS software platform, assessing their capabilities for volume rendering of structured grids\u2014a prevalent technique across various applications. The Apple Vision Pro supports multiple display modes, from classical augmented reality (AR) using video see-through technology to immersive virtual reality (VR) environments that exclusively render virtual objects. These modes utilize different APIs and exhibit distinct capabilities. Our focus is on direct volume rendering, selected for its implementation challenges due to the native graphics APIs being predominantly oriented towards surface shading. Volume rendering is particularly vital in fields where AR and VR visualizations offer substantial benefits, such as in medicine and manufacturing. Despite its initial high cost, we anticipate that the Vision Pro will become more accessible and affordable over time, following Apple\u2019s track record of market expansion. As these devices become more prevalent, understanding how to effectively program and utilize them becomes increasingly important, offering significant opportunities for innovation and practical applications in various sectors.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Let\u2019s Get Vysical: Perceptual Accuracy in Visual & Tactile Encodings",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00012",
    "authors": [
      {
        "name": "Zhongzheng Xu",
        "affiliation": "Emory University"
      },
      {
        "name": "Kristin Williams",
        "affiliation": "Emory University"
      },
      {
        "name": "Emily Wall",
        "affiliation": "Emory University"
      }
    ],
    "abstract": "In this paper, we explore the effectiveness of tactile data encodings using swell paper in comparison to visual encodings displayed with SVGs for data perception tasks. By replicating and adapting Cleveland and McGill\u2019s graphical perception study for the tactile modality, we establish a novel tactile encoding hierarchy. In a study with 12 university students, we found that participants perceived visual encodings more accurately when comparing values, judging their ratios with lower cognitive load, and better self-evaluated performance than tactile encodings. However, tactile encodings differed from their visual counterparts in terms of how accurately values could be decoded from them. This suggests that data physicalizations will require different design guidance than that developed for visual encodings. By providing empirical evidence for the perceptual accuracy of tactile encodings, our work contributes to foundational research on forms of data representation that prioritize tactile perception such as tactile graphics.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Line Harp: Importance-Driven Sonification for Dense Line Charts",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00046",
    "authors": [
      {
        "name": "Egil Bru",
        "affiliation": "University of Bergen, Norway"
      },
      {
        "name": "Thomas Trautner",
        "affiliation": "University of Bergen, Norway"
      },
      {
        "name": "Stefan Bruckner",
        "affiliation": "University of Rostock, Germany"
      }
    ],
    "abstract": "Accessibility in visualization is an important yet challenging topic. Sonification, in particular, is a valuable yet underutilized technique that can enhance accessibility for people with low vision. However, the lower bandwidth of the auditory channel makes it difficult to fully convey dense visualizations. For this reason, interactivity is key in making full use of its potential. In this paper, we present a novel approach for the sonification of dense line charts. We utilize the metaphor of a string instrument, where individual line segments can be \u201cplucked\u201d. We propose an importance-driven approach which encodes the directionality of line segments using frequency and dynamically scales amplitude for improved density perception. We discuss the potential of our approach based on a set of examples.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "LinkQ: An LLM-Assisted Visual Interface for Knowledge Graph Question-Answering",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00031",
    "authors": [
      {
        "name": "Harry Li",
        "affiliation": "MIT Lincoln Laboratory"
      },
      {
        "name": "Gabriel Appleby",
        "affiliation": "Tufts University"
      },
      {
        "name": "Ashley Suh",
        "affiliation": "MIT Lincoln Laboratory"
      }
    ],
    "abstract": "We present LinkQ, a system that leverages a large language model (LLM) to facilitate knowledge graph (KG) query construction through natural language question-answering. Traditional approaches often require detailed knowledge of a graph querying language, limiting the ability for users \u2013 even experts \u2013 to acquire valuable insights from KGs. LinkQ simplifies this process by implementing a multistep protocol in which the LLM interprets a user\u2019s question, then systematically converts it into a well-formed query. LinkQ helps users iteratively refine any open-ended questions into precise ones, supporting both targeted and exploratory analysis. Further, LinkQ guards against the LLM hallucinating outputs by ensuring users\u2019 questions are only ever answered from ground truth KG data. We demonstrate the efficacy of LinkQ through a qualitative study with five KG practitioners. Our results indicate that practitioners find LinkQ effective for KG question-answering, and desire future LLM-assisted exploratory data analysis systems.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Message from the VIS 2024 General Chairs",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00005",
    "authors": [],
    "abstract": "N/A",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Micro Visualizations on a Smartwatch: Assessing Reading Performance While Walking",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00017",
    "authors": [
      {
        "name": "Fairouz Grioui",
        "affiliation": "University of Stuttgart, Stuttgart, Germany"
      },
      {
        "name": "Tanja Blascheck",
        "affiliation": "University of Stuttgart, Stuttgart, Germany"
      },
      {
        "name": "Lijie Yao",
        "affiliation": "Universit\u00e9 Paris-Saclay CNRS, Inria, LISN, France"
      },
      {
        "name": "Petra Isenberg",
        "affiliation": "Universit\u00e9 Paris-Saclay CNRS, Inria, LISN, France"
      }
    ],
    "abstract": "With two studies, we assess how different walking trajectories (straight line, circular, and infinity) and speeds (2 km/h, 4 km/h, and 6 km/h) influence the accuracy and response time of participants reading micro visualizations on a smartwatch. We showed our participants common watch face micro visualizations including date, time, weather information, and four complications showing progress charts of fitness data. Our findings suggest that while walking trajectories did not significantly affect reading performance, overall walking activity, especially at high speeds, hurt reading accuracy and, to some extent, response time. Supplemental material is available at: https://osf.io/u78s6/.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "MinMaxLTTB: Leveraging MinMax-Preselection to Scale LTTB",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00013",
    "authors": [
      {
        "name": "Jeroen Van Der Donckt",
        "affiliation": "IDLab, Ghent University - imec, Belgium"
      },
      {
        "name": "Jonas Van Der Donckt",
        "affiliation": "IDLab, Ghent University - imec, Belgium"
      },
      {
        "name": "Michael Rademaker",
        "affiliation": "IDLab, Ghent University - imec, Belgium"
      },
      {
        "name": "Sofie Van Hoecke",
        "affiliation": "IDLab, Ghent University - imec, Belgium"
      }
    ],
    "abstract": "Visualization plays an important role in the analysis and exploration of time series data. To facilitate efficient visualization of large datasets, downsampling has emerged as a well-established approach. This work concentrates on LTTB (Largest-Triangle-Three-Buckets), a widely adopted downsampling algorithm for time series data point selection. Specifically, we introduce MinMaxLTTB, a two-step algorithm that significantly improves the scalability of LTTB. MinMaxLTTB consists of the following two steps: (i) the MinMax algorithm preselects a certain ratio of minimum and maximum data points, followed by (ii) applying the LTTB algorithm on only these preselected data points, effectively reducing LTTB\u2019s time complexity. The MinMax algorithm is computationally efficient and can be parallelized, enabling efficient data point preselection. Additionally, MinMax demonstrates competitive performance in terms of visual representation, making it also an effective data reduction method. Experimental results demonstrate that MinMaxLTTB outperforms LTTB by more than an order of magnitude in terms of computation time. Furthermore, preselecting a small multiple of the desired output size already yields similar visual representativeness compared to LTTB. In summary, MinMaxLTTB leverages the computational efficiency of MinMax to scale LTTB, without compromising on LTTB its favorable visualization properties. The code and experiments associated with this paper can be found at https://github.com/predict-idlab/MinMaxLTTB.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Multi-User Mobile Augmented Reality for Cardiovascular Surgical Planning",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00048",
    "authors": [
      {
        "name": "Pratham Mehta",
        "affiliation": "Georgia Institute of Technology"
      },
      {
        "name": "Rahul Narayanan",
        "affiliation": "Georgia Institute of Technology"
      },
      {
        "name": "Harsha Karanth",
        "affiliation": "Georgia Institute of Technology"
      },
      {
        "name": "Haoyang Yang",
        "affiliation": "Georgia Institute of Technology"
      },
      {
        "name": "Timothy C. Slesnick",
        "affiliation": "Children\u2019s Healthcare of Atlanta"
      },
      {
        "name": "Fawwaz Shaw",
        "affiliation": "Children\u2019s Healthcare of Atlanta"
      },
      {
        "name": "Duen Horng Chau",
        "affiliation": "Georgia Institute of Technology"
      }
    ],
    "abstract": "Collaborative planning for congenital heart diseases typically involves creating physical heart models through 3D printing, which are then examined by both surgeons and cardiologists. Recent developments in mobile augmented reality (AR) technologies have presented a viable alternative, known for their ease of use and portability. However, there is still a lack of research examining the utilization of multi-user mobile AR environments to support collaborative planning for cardiovascular surgeries. We created ARCOLLAB, an iOS AR app designed for enabling multiple surgeons and cardiologists to interact with a patient\u2019s 3D heart model in a shared environment. ARCOLLAB enables surgeons and cardiologists to import heart models, manipulate them through gestures and collaborate with other users, eliminating the need for fabricating physical heart models. Our evaluation of ARCOL-LAB\u2019s usability and usefulness in enhancing collaboration, conducted with three cardiothoracic surgeons and two cardiologists, marks the first human evaluation of a multi-user mobile AR tool for surgical planning. ARCOLLAB is open-source, available at https://github.com/poloclub/arcollab.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "On Combined Visual Cluster and Set Analysis",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00034",
    "authors": [
      {
        "name": "Nikolaus Piccolotto",
        "affiliation": "TU Wien"
      },
      {
        "name": "Markus Wallinger",
        "affiliation": "TU Wien"
      },
      {
        "name": "Silvia Miksch",
        "affiliation": "TU Wien"
      },
      {
        "name": "Markus B\u00f6gl",
        "affiliation": "TU Wien"
      }
    ],
    "abstract": "Real-world datasets often consist of quantitative and categorical variables. The analyst needs to focus on either kind separately or both jointly. We proposed a visualization technique tackling these challenges that supports visual cluster and set analysis. In this paper, we investigate how its visualization parameters affect the accuracy and speed of cluster and set analysis tasks in a controlled experiment. Our findings show that, with the proper settings, our visualization can support both task types well. However, we did not find settings suitable for the joint task, which provides opportunities for future research.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Opening the Black Box of 3D Reconstruction Error Analysis with VECTOR",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00065",
    "authors": [
      {
        "name": "Racquel Fygenson",
        "affiliation": "Northeastern University"
      },
      {
        "name": "Kazi Jawad",
        "affiliation": "W\u0113t\u0101 FX"
      },
      {
        "name": "Isabel Li",
        "affiliation": "Art Center College of Design"
      },
      {
        "name": "Francois Ayoub",
        "affiliation": "Jet Propulsion Laboratory, California Institute of Technology"
      },
      {
        "name": "Robert G. Deen",
        "affiliation": "Jet Propulsion Laboratory, California Institute of Technology"
      },
      {
        "name": "Scott Davidoff",
        "affiliation": "Jet Propulsion Laboratory, California Institute of Technology"
      },
      {
        "name": "Dominik Moritz",
        "affiliation": "Carnegie Mellon University"
      },
      {
        "name": "Mauricio Hess-Flores",
        "affiliation": "Jet Propulsion Laboratory, California Institute of Technology"
      }
    ],
    "abstract": "Reconstruction of 3D scenes from 2D images is a technical challenge that impacts domains from Earth and planetary sciences and space exploration to augmented and virtual reality. Typically, reconstruction algorithms first identify common features across images and then minimize reconstruction errors after estimating the shape of the terrain. This bundle adjustment (BA) step optimizes around a single, simplifying scalar value that obfuscates many possible causes of reconstruction errors (e.g., initial estimate of the position and orientation of the camera, lighting conditions, ease of feature detection in the terrain). Reconstruction errors can lead to inaccurate scientific inferences or endanger a spacecraft exploring a remote environment. To address this challenge, we present VECTOR, a visual analysis tool that improves error inspection for stereo reconstruction BA. VECTOR provides analysts with previously unavailable visibility into feature locations, camera pose, and computed 3D points. VECTOR was developed in partnership with the Perseverance Mars Rover and Ingenuity Mars Helicopter terrain reconstruction team at the NASA Jet Propulsion Laboratory. We report on how this tool was used to debug and improve terrain reconstruction for the Mars 2020 mission.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Projection Ensemble: Visualizing the Robust Structures of Multidimensional Projections",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00018",
    "authors": [
      {
        "name": "Myeongwon Jung",
        "affiliation": "Sungkyunkwan University"
      },
      {
        "name": "Jiwon Choi",
        "affiliation": "Sungkyunkwan University"
      },
      {
        "name": "Jaemin Jo",
        "affiliation": "Sungkyunkwan University"
      }
    ],
    "abstract": "We introduce Projection Ensemble, a novel approach for identifying and visualizing robust structures across multidimensional projections. Although multidimensional projections, such as t-Stochastic Neighbor Embedding (t-SNE), have gained popularity, their stochastic nature often leads the user to interpret the structures that arise by chance and make erroneous findings. To overcome this limitation, we present a frequent subgraph mining algorithm and a visualization interface to extract and visualize the consistent structures across multiple projections. We demonstrate that our system not only identifies trustworthy structures but also detects accidental clustering or separation of data points.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "ProtoGraph: A Non-Expert Toolkit for Creating Animated Graphs",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00044",
    "authors": [
      {
        "name": "Malchiel Rodrigues",
        "affiliation": "Harvard University"
      },
      {
        "name": "Joel Dapello",
        "affiliation": "Harvard University"
      },
      {
        "name": "Priyan Vaithilingam",
        "affiliation": "Harvard University"
      },
      {
        "name": "Carolina Nobre",
        "affiliation": "Harvard University"
      },
      {
        "name": "Johanna Beyer",
        "affiliation": "Harvard University"
      }
    ],
    "abstract": "Creating intuitive and aesthetically pleasing visualizations and animations of small-to-moderate-sized graphs in the form of node-link diagrams is a common task across many fields, particularly in pedagogical settings. However, creating a graph visualization either requires users to manually construct a graph by hand or programming skills. We present ProtoGraph, an English-like programming language for non-expert users to rapidly specify and animate node-link graph visualizations. The language supports iterative prototyping, thereby allowing non-experts users to intuitively refine their graphs, and to easily create animated graphs. The key features of ProtoGraph include a web-based live coding interface, previews for the different states in an animated graph, integrated user documentation, and an active-learning style tutorial. We have integrated the ProtoGraph language into an open-source JavaScript graph visualization library for rendering and a graphical web interface for rapid prototyping. In a user study, we show that participants with varying coding experiences were able to quickly learn the ProtoGraph language and create real-world pedagogical visualizations, showing that ProtoGraph is easy to learn, efficient to use, and extensible.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "PyGWalker: On-the-fly Assistant for Exploratory Visual Data Analysis",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00009",
    "authors": [
      {
        "name": "Yue Yu",
        "affiliation": "Hong Kong University of Science and Technology"
      },
      {
        "name": "Leixian Shen",
        "affiliation": "Hong Kong University of Science and Technology"
      },
      {
        "name": "Fei Long",
        "affiliation": "Kanaries Data"
      },
      {
        "name": "Huamin Qu",
        "affiliation": "Hong Kong University of Science and Technology"
      },
      {
        "name": "Hao Chen",
        "affiliation": "Kanaries Data"
      }
    ],
    "abstract": "Exploratory visual data analysis tools empower data analysts to efficiently and intuitively explore data insights throughout the entire analysis cycle. However, the gap between common programmatic analysis (e.g., within computational notebooks) and exploratory visual analysis leads to a disjointed and inefficient data analysis experience. To bridge this gap, we developed PyGWalker, a Python library that offers on-the-fly assistance for exploratory visual data analysis. It features a lightweight and intuitive GUI with a shelf builder modality. Its loosely coupled architecture supports multiple computational environments to accommodate varying data sizes. Since its release in February 2023, PyGWalker has gained much attention, with 612k downloads on PyPI and over 10.5k stars on GitHub as of June 2024. This demonstrates its value to the data science and visualization community, with researchers and developers integrating it into their own applications and studies.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Quantifying the Impact of XR Visual Guidance on User Performance Using a Large-Scale Virtual Assembly Experiment",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00051",
    "authors": [
      {
        "name": "Leon Pietschmann",
        "affiliation": "University of Cambridge"
      },
      {
        "name": "Paul Z\u00fcrcher",
        "affiliation": "University of Cambridge"
      },
      {
        "name": "Erik Bubik",
        "affiliation": "University of Cambridge"
      },
      {
        "name": "Zhutian Chen",
        "affiliation": "Harvard University"
      },
      {
        "name": "Hanspeter Pfister",
        "affiliation": "Harvard University"
      },
      {
        "name": "Thomas Bohn\u00e9",
        "affiliation": "Harvard University"
      }
    ],
    "abstract": "The combination of Visual Guidance and Extended Reality (XR) technology holds the potential to greatly improve the performance of human workforces in numerous areas, particularly industrial environments. Focusing on virtual assembly tasks and making use of different forms of supportive visualisations, this study investigates the potential of XR Visual Guidance. Set in a web-based immersive environment, our results draw from a heterogeneous pool of 199 participants. This research is designed to significantly differ from previous exploratory studies, which yielded conflicting results on user performance and associated human factors. Our results clearly show the advantages of XR Visual Guidance based on an over 50% reduction in task completion times and mistakes made; this may further be enhanced and refined using specific frameworks and other forms of visualisations/Visual Guidance. Discussing the role of other factors, such as cognitive load, motivation, and usability, this paper also seeks to provide concrete avenues for future research and practical takeaways for practitioners.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Representing Charts as Text for Language Models: An In-Depth Study of Question Answering for Bar Charts",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00061",
    "authors": [
      {
        "name": "Victor Soares Bursztyn",
        "affiliation": "Adobe Research"
      },
      {
        "name": "Jane Hoffswell",
        "affiliation": "Adobe Research"
      },
      {
        "name": "Eunyee Koh",
        "affiliation": "Adobe Research"
      },
      {
        "name": "Shunan Guo",
        "affiliation": "Adobe Research"
      }
    ],
    "abstract": "Machine Learning models for chart-grounded Q&A (CQA) often treat charts as images, but performing CQA on pixel values has proven challenging. We thus investigate a resource overlooked by current ML-based approaches: the declarative documents describing how charts should visually encode data (i.e., chart specifications). In this work, we use chart specifications to enhance language models (LMs) for chart-reading tasks, such that the resulting system can robustly understand language for CQA. Through a case study with 359 bar charts, we test novel fine tuning schemes on both GPT-3 and T5 using a new dataset curated for two CQA tasks: question-answering and visual explanation generation. Our text-only approaches strongly outperform vision-based GPT-4 on explanation generation (99% vs. 63% accuracy), and show promising results for question-answering (57\u201367% accuracy). Through in-depth experiments, we also show that our text-only approaches are mostly robust to natural language variation.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "ScatterUQ: Interactive Uncertainty Visualizations for Multiclass Deep Learning Problems",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00058",
    "authors": [
      {
        "name": "Harry X. Li",
        "affiliation": "MIT Lincoln Laboratory"
      },
      {
        "name": "Steven Jorgensen",
        "affiliation": "MIT Lincoln Laboratory"
      },
      {
        "name": "John Holodnak",
        "affiliation": "MIT Lincoln Laboratory"
      },
      {
        "name": "Allan B. Wollaber",
        "affiliation": "MIT Lincoln Laboratory"
      }
    ],
    "abstract": "Recently, uncertainty-aware deep learning methods for multiclass labeling problems have been developed that provide calibrated class prediction probabilities and out-of-distribution (OOD) indicators, letting machine learning (ML) consumers and engineers gauge a model\u2019s confidence in its predictions. However, this extra neural network prediction information is challenging to scalably convey visually for arbitrary data sources under multiple uncertainty contexts. To address these challenges, we present ScatterUQ, an interactive system that provides targeted visualizations to allow users to better understand model performance in context-driven uncertainty settings. ScatterUQ leverages recent advances in distance-aware neural networks, together with dimensionality reduction techniques, to construct robust, 2-D scatter plots explaining why a model predicts a test example to be (1) in-distribution and of a particular class, (2) in-distribution but unsure of the class, and (3) out-of-distribution. ML consumers and engineers can visually compare the salient features of test samples with training examples through the use of a \u201chover callback\u201d to understand model uncertainty performance and decide follow up courses of action. We demonstrate the effectiveness of ScatterUQ to explain model uncertainty for a multiclass image classification on a distance-aware neural network trained on Fashion-MNIST and tested on Fashion-MNIST (in distribution) and MNIST digits (out of distribution), as well as a deep learning model for a cyber dataset. We quantitatively evaluate dimensionality reduction techniques to optimize our contextually driven UQ visualizations. Our results indicate that the ScatterUQ system should scale to arbitrary, multiclass datasets. Our code is available at https://github.com/mit-ll-responsible-ai/equine-webapp.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Science in a Blink: Supporting Ensemble Perception in Scalar Fields",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00051",
    "authors": [
      {
        "name": "Victor A. Mateevitsi",
        "affiliation": "Argonne National Laboratory, University of Illinois Chicago"
      },
      {
        "name": "Michael E. Papka",
        "affiliation": "Argonne National Laboratory, University of Illinois Chicago"
      },
      {
        "name": "Khairi Reda",
        "affiliation": "Indiana University Indianapolis"
      }
    ],
    "abstract": "Visualizations support rapid analysis of scientific datasets, allowing viewers to glean aggregate information (e.g., the mean) within split-seconds. While prior research has explored this ability in conventional charts, it is unclear if spatial visualizations used by computational scientists afford a similar ensemble perception capacity. We investigate people\u2019s ability to estimate two summary statistics, mean and variance, from pseudocolor scalar fields. In a crowd- sourced experiment, we find that participants can reliably characterize both statistics, although variance discrimination requires a much stronger signal. Multi-hue and diverging colormaps outperformed monochromatic, luminance ramps in aiding this extraction. Analysis of qualitative responses suggests that participants often estimate the distribution of hotspots and valleys as visual proxies for data statistics. These findings suggest that people\u2019s summary interpretation of spatial datasets is likely driven by the appearance of discrete color segments, rather than assessments of overall luminance. Implicit color segmentation in quantitative displays could thus prove more useful than previously assumed by facilitating quick, gist- level judgments about color-coded visualizations.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Show Me My Users: A Dashboard Visualizing User Interaction Logs",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00040",
    "authors": [
      {
        "name": "Jinrui Wang",
        "affiliation": "The University of Edinburgh"
      },
      {
        "name": "Mashael AlKadi",
        "affiliation": "The University of Edinburgh"
      },
      {
        "name": "Benjamin Bach",
        "affiliation": "The University of Edinburgh"
      }
    ],
    "abstract": "This paper describes the design of a dashboard and analysis pipeline to monitor users of visualization tools in the wild. Our pipeline describes how to extract analysis KPIs from extensive log event data and a mix of user types. The resulting three-page dashboard displays live KPIs, helping analysts to understand users, detect exploratory behaviors, plan education interventions, and improve tool features. We propose this case study as a motivation to use the dashboard approach for a more \u2018casual\u2019 monitoring of users and building carer mindsets for visualization tools.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Simulating the Geometric Growth of the Marine Sponge Crella Incrustans",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00032",
    "authors": [
      {
        "name": "Joshua O\u2019Hagan",
        "affiliation": "Computational Media Innovation Centre, Victoria University of Wellington"
      },
      {
        "name": "Andrew Chalmers",
        "affiliation": "Computational Media Innovation Centre, Victoria University of Wellington"
      },
      {
        "name": "Taehyun Rhee",
        "affiliation": "Computational Media Innovation Centre, Victoria University of Wellington"
      }
    ],
    "abstract": "Simulating marine sponge growth helps marine biologists analyze, measure, and predict the effects that the marine environment has on marine sponges, and vice versa. This paper describes a way to simulate and grow geometric models of the marine sponge Crella incrustans while considering environmental factors including fluid flow and nutrients. The simulation improves upon prior work by changing the skeletal architecture of the sponge in the growth model to better suit the structure of Crella incrustans. The change in skeletal architecture and other simulation parameters are then evaluated qualitatively against photos of a real-life Crella incrustans sponge. The results support the hypothesis that changing the skeletal architecture from radiate accretive to Halichondrid produces a sponge model which is closer in resemblance to Crella incrustans than the prior work.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Table of Contents",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00004",
    "authors": [],
    "abstract": "N/A",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Table of Contents",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00004",
    "authors": [],
    "abstract": "N/A",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Taken By Surprise? Evaluating how Bayesian Surprise & Suppression Influences Peoples\u2019 Takeaways in Map Visualizations",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00036",
    "authors": [
      {
        "name": "Akim Ndlovu",
        "affiliation": "Worcester Polytechnic Institute"
      },
      {
        "name": "Hilson Shrestha",
        "affiliation": "Worcester Polytechnic Institute"
      },
      {
        "name": "Lane T. Harrison",
        "affiliation": "Worcester Polytechnic Institute"
      }
    ],
    "abstract": "Choropleth maps have been studied and extended in many ways to counteract the many biases that can occur when using them. Two recent techniques, Surprise metrics and Value Suppressing Uncertainty Palettes (VSUPs), offer promising solutions but have yet to be tested empirically with users of visualizations. In this paper, we explore how well people can make use of these techniques in map exploration tasks. We report a crowdsourced experiment where n = 300 participants are assigned to one of Choropleth, Surprise (only), and VSUP conditions (depicting rates and Surprise in a suppressed palette). Results show clear differences in map analysis outcomes, e.g. with Surprise maps leading people to significantly higher areas of population, or VSUPs performing similar or better than Choropleths for rate selection. Qualitative analysis suggests that many participants may only consider a subset of the metrics presented to them during exploration and decision-making. We discuss how these results generally support the use of Surprise and VSUP techniques in practice, and opportunities for further technique development. The material for the study (data, study results and code) is publicly available on https://osf.io/exb95/.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Text-based transfer function design for semantic volume rendering",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00047",
    "authors": [
      {
        "name": "Sangwon Jeong",
        "affiliation": "Vanderbilt University"
      },
      {
        "name": "Jixian Li",
        "affiliation": "SCI Institute, University of Utah"
      },
      {
        "name": "Chris R. Johnson",
        "affiliation": "SCI Institute, University of Utah"
      },
      {
        "name": "Shusen Liu",
        "affiliation": "Lawrence Livermore National Laboratory"
      },
      {
        "name": "Matthew Berger",
        "affiliation": "Vanderbilt University"
      }
    ],
    "abstract": "Transfer function design is crucial in volume rendering, as it directly influences the visual representation and interpretation of volumetric data. However, creating effective transfer functions that align with users\u2019 visual objectives is often challenging due to the complex parameter space and the semantic gap between transfer function values and features of interest within the volume. In this work, we propose a novel approach that leverages recent advancements in language-vision models to bridge this semantic gap. By employing a fully differentiable rendering pipeline and an image-based loss function guided by language descriptions, our method generates transfer functions that yield volume-rendered images closely matching the user\u2019s intent. We demonstrate the effectiveness of our approach in creating meaningful transfer functions from simple descriptions, empowering users to intuitively express their desired visual outcomes with minimal effort. This advancement streamlines the transfer function design process and makes volume rendering more accessible to a wider range of users.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "The Comic Construction Kit: An Activity for Students to Learn and Explain Data Visualizations",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00025",
    "authors": [
      {
        "name": "Magdalena Boucher",
        "affiliation": "St. P\u00f6lten University of Applied Sciences"
      },
      {
        "name": "Christina Stoiber",
        "affiliation": "St. P\u00f6lten University of Applied Sciences"
      },
      {
        "name": "Mandy Keck",
        "affiliation": "University of Applied Sciences, Upper Austria"
      },
      {
        "name": "Victor-Adriel De-Jesus-Oliveira",
        "affiliation": "St. P\u00f6lten University of Applied Sciences"
      },
      {
        "name": "Wolfgang Aigner",
        "affiliation": "St. P\u00f6lten University of Applied Sciences"
      }
    ],
    "abstract": "As visualization literacy and its implications gain prominence, we need effective methods to prepare students for the variety of visualizations in an increasingly data-driven world. Recently, the potential of comics has been recognized in various data visualization contexts, including educational settings. We describe the development of a workshop in which we use our \"comic construction kit\" as a tool for students to understand various data visualization techniques through an interactive creative approach of creating explanatory comics. We report on our insights from holding eight workshops with high school students and teachers, university students, and lecturers, aiming to enhance the landscape of handson visualization activities that can enrich the visualization classroom. The comic construction kit and all supplemental materials are open source under a CC-BY license and available at https://fhstp.github.io/comixplain/vis4schools.html.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "The Role of Visualization in Genomics Data Analysis Workflows: The Interviews",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00029",
    "authors": [
      {
        "name": "Sehi L\u2019Yi",
        "affiliation": "Harvard Medical School"
      },
      {
        "name": "Qianwen Wang",
        "affiliation": "Harvard Medical School"
      },
      {
        "name": "Nils Gehlenborg",
        "affiliation": "Harvard Medical School"
      }
    ],
    "abstract": "The diversity of genome-mapped data and analysis tasks makes it challenging for a single visualization tool to fulfill all visualization needs. To design a visualization tool that supports various genomics workflows of users, it is critical to first gain insights into the diverse workflows and the limitations of existing genomics tools for supporting them. In this paper, we conducted semi-structured interviews (N=9) to understand the role of visualization in genomics data analysis workflows. Our main goals were to identify various genomics workflows, from data analysis to visual exploration and presentation, and to observe challenges that genomics analysts encounter in these workflows when using existing tools. Through the interviews, we found several unique characteristics of genomics workflows, such as the use of multiple visualization tools and many repetitive tasks, which can significantly affect the overall performance. Based on our findings, we discuss implications for designing effective visualization authoring tools that tightly support genomics workflows, such as supporting automation and reproducibility.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "TimePool: Visually Answer \u201cWhich and When\u201d Questions On Univariate Time Series",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00049",
    "authors": [
      {
        "name": "Tinghao Feng",
        "affiliation": "Appalachian State University"
      },
      {
        "name": "Yueqi Hu",
        "affiliation": "UNC Charlotte"
      },
      {
        "name": "Jing Yang",
        "affiliation": "UNC Charlotte"
      },
      {
        "name": "Tom Polk",
        "affiliation": "Compass Draw, LLC"
      },
      {
        "name": "Ye Zhao",
        "affiliation": "Kent State University"
      },
      {
        "name": "Shixia Liu",
        "affiliation": "Tshinghua University"
      },
      {
        "name": "Zhaocong Yang",
        "affiliation": "Appalachian State University"
      }
    ],
    "abstract": "When exploring time series datasets, analysts often pose \u201cwhich and when\u201d questions. For example, with world life expectancy data over one hundred years, they may inquire about the top 10 countries in life expectancy and the time period when they achieved this status, or which countries have had longer life expectancy than Ireland and when. This paper proposes TimePool, a new visualization prototype, to address this need for univariate time series analysis. It allows users to construct interactive \u201cwhich and when\u201d queries and visually explore the results for insights. TimePool has been evaluated through example scenarios, a formal user study, and interviews with domain experts.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Title Page i",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00001",
    "authors": [],
    "abstract": "N/A",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Title Page i",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00001",
    "authors": [],
    "abstract": "N/A",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Title Page iii",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00002",
    "authors": [],
    "abstract": "N/A",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Title Page iii",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00002",
    "authors": [],
    "abstract": "N/A",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Topological Analysis and Approximate Identification of Leading Lines in Artworks Based on Discrete Morse Theory",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00010",
    "authors": [
      {
        "name": "Fuminori Shibasaki",
        "affiliation": "Keio University, Japan"
      },
      {
        "name": "Issei Fujishiro",
        "affiliation": "Keio University, Japan"
      }
    ],
    "abstract": "Accomplished artists often incorporate leading lines into their compositions to guide the observer\u2019s attention. Although saliency maps are typically employed to locate attractive regions in still images, such scalar features do not express the trajectory of an observer\u2019s gaze. In this study, we propose a method for the visual analysis and approximate identification of leading lines based on maximum graphs, sparse subsets of Morse\u2013Smale complexes, extracted from saliency maps. We provide empirical evidence substantiating the feasibility of our method through a comparison with actual observers\u2019 eye tracking results. Further, we investigate the limitations of our approach by employing it to analyze a variety of artworks with diverse styles.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Topological Separation of Vortices",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00070",
    "authors": [
      {
        "name": "Adeel Zafar",
        "affiliation": "University of Houston"
      },
      {
        "name": "Zahra Poorshayegh",
        "affiliation": "University of Houston"
      },
      {
        "name": "Di Yang",
        "affiliation": "University of Houston"
      },
      {
        "name": "Guoning Chen",
        "affiliation": "University of Houston"
      }
    ],
    "abstract": "Vortices and their analysis play a critical role in the understanding of complex phenomena in turbulent flows. Traditional vortex extraction methods, notably region-based techniques, often overlook the entanglement phenomenon, resulting in the inclusion of multiple vortices within a single extracted region. Their separation is necessary for quantifying different types of vortices and their statistics. In this study, we propose a novel vortex separation method that extends the conventional contour tree-based segmentation approach with an additional step termed \"layering\". Upon extracting a vortical region using specified vortex criteria (e.g., \u03bb2), we initially establish topological segmentation based on the contour tree, followed by the layering process to allocate appropriate segmentation IDs to unsegmented cells, thus separating individual vortices within the region. However, these regions may still suffer from inaccurate splits, which we address statistically by leveraging the continuity of vorticity lines across the split boundaries. Our findings demonstrate a significant improvement in both the separation of vortices and the mitigation of inaccurate splits compared to prior methods.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Towards Autocomplete Strategies for Visualization Construction",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00037",
    "authors": [
      {
        "name": "Wei Wei",
        "affiliation": "T\u00e9l\u00e9com Paris, University of Victoria University of Calgary"
      },
      {
        "name": "Samuel Huron",
        "affiliation": "T\u00e9l\u00e9com Paris, CNRS i3, Institut Polytechnique de Paris"
      },
      {
        "name": "Yvonne Jansen",
        "affiliation": "CNRS, Inria, LaBRI, Univ. Bordeaux"
      }
    ],
    "abstract": "Constructive visualization uses physical data units - tokens - to enable non-experts to create personalized visualizations engagingly. However, its physical nature limits efficiency and scalability. One potential solution to address this issue is autocomplete. By providing automated suggestions while still allowing for manual intervention, autocomplete can expedite visualization construction while maintaining expressivity. We conduct a speculative design study to examine how people would like to interact with a visualization authoring system that supports autocomplete. Our study identifies three types of autocomplete strategies and gains insights for designing future visualization authoring tools with autocomplete functionality.A free copy of this paper and all supplemental materials are available on our online repository: https://osf.io/nu4z3/?view_only= 594baee54d114a99ab381886fb32a126.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Towards Reusable and Reactive Widgets for Information Visualization Research and Dissemination",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00071",
    "authors": [
      {
        "name": "John A. Guerra-Gomez",
        "affiliation": "Northeastern University"
      }
    ],
    "abstract": "The information visualization research community commonly produces supporting software to demonstrate technical contributions to the field. However, developing this software tends to be an overwhelming task. The final product tends to be a research prototype without much thought for modularization and re-usability, which makes it harder to replicate and adopt. This paper presents a design pattern for facilitating the creation, dissemination, and re-utilization of visualization techniques using reactive widgets. The design pattern features basic concepts that leverage modern front-end development best practices and standards, which facilitate development and replication. The paper presents several usage examples of the pattern, templates for implementation, and even a wrapper for facilitating the conversion of any Vega [27], [28] specification into a reactive widget. https://johnguerra.co/reactiveWidgets",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Towards a Quality Approach to Hierarchical Color Maps",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00052",
    "authors": [
      {
        "name": "Tobias Mertz",
        "affiliation": "Fraunhofer IGD"
      },
      {
        "name": "J\u00f6rn Kohlhammer",
        "affiliation": "Fraunhofer IGD, TU Darmstadt"
      }
    ],
    "abstract": "To improve the perception of hierarchical structures in data sets, several color map generation algorithms have been proposed to take this structure into account. But the design of hierarchical color maps elicits different requirements to those of color maps for tabular data. Within this paper, we make an initial effort to put design rules from the color map literature into the context of hierarchical color maps. We investigate the impact of several design decisions and provide recommendations for various analysis scenarios. Thus, we lay the foundation for objective quality criteria to evaluate hierarchical color maps.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Two-point Equidistant Projection and Degree-of-interest Filtering for Smooth Exploration of Geo-referenced Networks",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00023",
    "authors": [
      {
        "name": "Max Franke",
        "affiliation": "University of Stuttgart, Germany"
      },
      {
        "name": "Samuel Beck",
        "affiliation": "University of Stuttgart, Germany"
      },
      {
        "name": "Steffen Koch",
        "affiliation": "University of Stuttgart, Germany"
      }
    ],
    "abstract": "The visualization and interactive exploration of geo-referenced networks poses challenges if the network\u2019s nodes are not evenly distributed. Our approach proposes new ways of realizing animated transitions for exploring such networks from an ego-perspective. We aim to reduce the required screen estate while maintaining the viewers\u2019 mental map of distances and directions. A preliminary study provides first insights of the comprehensiveness of animated geographic transitions regarding directional relationships between start and end point in different projections. Two use cases showcase how ego-perspective graph exploration can be supported using less screen space than previous approaches.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Uniform Sample Distribution in Scatterplots via Sector-based Transformation",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00039",
    "authors": [
      {
        "name": "Hennes Rave",
        "affiliation": "University of M\u00fcnster, Germany"
      },
      {
        "name": "Vladimir Molchanov",
        "affiliation": "University of M\u00fcnster, Germany"
      },
      {
        "name": "Lars Linsen",
        "affiliation": "University of M\u00fcnster, Germany"
      }
    ],
    "abstract": "A high number of samples often leads to occlusion in scatter-plots, which hinders data perception and analysis. De-cluttering approaches based on spatial transformation reduce visual clutter by remapping samples using the entire available scatterplot domain. Such regularized scatterplots may still be used for data analysis tasks, if the spatial transformation is smooth and preserves the original neighborhood relations of samples. Recently, Rave et al. [21] proposed an efficient regularization method based on integral images. We propose a generalization of their regularization scheme using sector-based transformations with the aim of increasing sample uniformity of the resulting scatterplot. We document the improvement of our approach using various uniformity measures.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Use-Coordination: Model, Grammar, and Library for Implementation of Coordinated Multiple Views",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00041",
    "authors": [
      {
        "name": "Mark S. Keller",
        "affiliation": "Harvard Medical School"
      },
      {
        "name": "Trevor Manz",
        "affiliation": "Harvard Medical School"
      },
      {
        "name": "Nils Gehlenborg",
        "affiliation": "Harvard Medical School"
      }
    ],
    "abstract": "Coordinated multiple views (CMV) in a visual analytics system can help users explore multiple data representations simultaneously with linked interactions. However, the implementation of coordinated multiple views can be challenging. Without standard software libraries, visualization designers need to re-implement CMV during the development of each system. We introduce use-coordination, a grammar and software library that supports the efficient implementation of CMV. The grammar defines a JSON-based representation for an abstract coordination model from the information visualization literature. We contribute an optional extension to the model and grammar that allows for hierarchical coordination. Through three use cases, we show that use-coordinationenables implementation of CMV in systems containing not only basic statistical charts but also more complex visualizations such as medical imaging volumes. We describe six software extensions, including a graphical editor for manipulation of coordination, which showcase the potential to build upon our coordination-focused declarative approach. The software is open-source and available at https://use-coordination.dev.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "VIS 2023 Conference Committee",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00006",
    "authors": [],
    "abstract": "N/A",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "VIS 2023 Program Committee",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00007",
    "authors": [],
    "abstract": "N/A",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "VIS 2023 Reviewers",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00008",
    "authors": [],
    "abstract": "N/A",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "VIS 2024 Conference Committee",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00006",
    "authors": [],
    "abstract": "N/A",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "VIS 2024 Program Committee",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00007",
    "authors": [],
    "abstract": "N/A",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "VIS 2024 Reviewers",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00008",
    "authors": [],
    "abstract": "N/A",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Vis-SPLIT: Interactive Hierarchical Modeling for mRNA Expression Classification",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00030",
    "authors": [
      {
        "name": "Braden Roper",
        "affiliation": "University of Oklahoma"
      },
      {
        "name": "James C. Mathews",
        "affiliation": "Memorial Sloan Kettering Cancer Center"
      },
      {
        "name": "Saad Nadeem",
        "affiliation": "Memorial Sloan Kettering Cancer Center"
      },
      {
        "name": "Ji Hwan Park",
        "affiliation": "University of Oklahoma"
      }
    ],
    "abstract": "We propose an interactive visual analytics tool, Vis-SPLIT, for partitioning a population of individuals into groups with similar gene signatures. Vis-SPLIT allows users to interactively explore a dataset and exploit visual separations to build a classification model for specific cancers. The visualization components reveal gene expression and correlation to assist specific partitioning decisions, while also providing overviews for the decision model and clustered genetic signatures. We demonstrate the effectiveness of our framework through a case study and evaluate its usability with domain experts. Our results show that Vis-SPLIT can classify patients based on their genetic signatures to effectively gain insights into RNA sequencing data, as compared to an existing classification system.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Visual Analysis of Large Multi-Field AMR Data on GPUs Using Interactive Volume Lines",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00020",
    "authors": [
      {
        "name": "Stefan Zellmann",
        "affiliation": "University of Cologne"
      },
      {
        "name": "Serkan Demirci",
        "affiliation": "Bilkent University"
      },
      {
        "name": "U\u011fur G\u00fcd\u00fckbay",
        "affiliation": "Bilkent University"
      }
    ],
    "abstract": "To visually compare ensembles of volumes, dynamic volume lines (DVLs) represent each ensemble member as a 1D polyline. To compute these, the volume cells are sorted on a space-filling curve and scaled by the ensemble\u2019s local variation. The resulting 1D plot can augment or serve as an alternative to a 3D volume visualization free of visual clutter and occlusion. Interactively computing DVLs is challenging when the data is large, and the volume grid is not structured/regular, as is often the case with computational fluid dynamics simulations. We extend DVLs to support large-scale, multifield adaptive mesh refinement (AMR) data that can be explored interactively. Our GPU-based system updates the DVL representation whenever the data or the alpha transfer function changes. We demonstrate and evaluate our interactive prototype using large AMR volumes from astrophysics simulations.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Visual Validation versus Visual Estimation: A Study on the Average Value in Scatterplots",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00045",
    "authors": [
      {
        "name": "Daniel Braun",
        "affiliation": "University of Cologne"
      },
      {
        "name": "Ashley Suh",
        "affiliation": "Tufts University"
      },
      {
        "name": "Remco Chang",
        "affiliation": "Tufts University"
      },
      {
        "name": "Michael Gleicher",
        "affiliation": "University of Wisconsin, Madison"
      },
      {
        "name": "Tatiana von Landesberger",
        "affiliation": "University of Cologne"
      }
    ],
    "abstract": "We investigate the ability of individuals to visually validate statistical models in terms of their fit to the data. While visual model estimation has been studied extensively, visual model validation remains under-investigated. It is unknown how well people are able to visually validate models, and how their performance compares to visual and computational estimation. As a starting point, we conducted a study across two populations (crowdsourced and volunteers). Participants had to both visually estimate (i.e, draw) and visually validate (i.e., accept or reject) the frequently studied model of averages. Across both populations, the level of accuracy of the models that were considered valid was lower than the accuracy of the estimated models. We find that participants\u2019 validation and estimation were unbiased. Moreover, their natural critical point between accepting and rejecting a given mean value is close to the boundary of its 95% confidence interval, indicating that the visually perceived confidence interval corresponds to a common statistical standard. Our work contributes to the understanding of visual model validation and opens new research opportunities.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Visualization of 2D Scalar Field Ensembles Using Volume Visualization of the Empirical Distribution Function",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00046",
    "authors": [
      {
        "name": "Tomas Daetz",
        "affiliation": "Leipzig University"
      },
      {
        "name": "Michael B\u00f6ttinger",
        "affiliation": "German Climate Computing Center"
      },
      {
        "name": "Gerik Scheuermann",
        "affiliation": "Leipzig University"
      },
      {
        "name": "Christian Heine",
        "affiliation": "Leipzig University"
      }
    ],
    "abstract": "Analyzing uncertainty in spatial data is a vital task in many domains, as for example with climate and weather simulation ensembles. Although many methods support the analysis of uncertain 2D data, such as uncertain isocontours or overlaying of statistical information on plots of the actual data, it is still a challenge to get a more detailed overview of 2D data together with its statistical properties. We present cumulative height fields, a visualization method for 2D scalar field ensembles using the marginal empirical distribution function and show preliminary results using volume rendering and slicing for the Max Planck Institute Grand Ensemble.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Visualizations on Smart Watches while Running: It Actually Helps!",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00016",
    "authors": [
      {
        "name": "Sarina Kashanj",
        "affiliation": "University of Victoria"
      },
      {
        "name": "Xiyao Wang",
        "affiliation": "University of Victoria"
      },
      {
        "name": "Charles Perin",
        "affiliation": "University of Victoria"
      }
    ],
    "abstract": "Millions of runners rely on smart watches that display running-related metrics such as pace, heart rate and distance for training and racing \u2014 mostly with text and numbers. Although research tells us that visualizations are a good alternative to text on smart watches, we know little about how visualizations can help in realistic running scenarios. We conducted a study in which 20 runners completed running-related tasks on an outdoor track using both text and visualizations. Our results show that runners are 1.5 to 8 times faster in completing those tasks with visualizations than with text, prefer visualizations to text, and would use such visualizations while running \u2014 if available on their smart watch.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Visualizing Linguistic Diversity of Text Datasets Synthesized by Large Language Models",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00056",
    "authors": [
      {
        "name": "Emily Reif",
        "affiliation": "Google Research"
      },
      {
        "name": "Minsuk Kahng",
        "affiliation": "Google Research"
      },
      {
        "name": "Savvas Petridis",
        "affiliation": "Google Research"
      }
    ],
    "abstract": "Large language models (LLMs) can be used to generate smaller, more refined datasets via few-shot prompting for benchmarking, fine-tuning or other use cases. However, understanding and evaluating these datasets is difficult, and the failure modes of LLM-generated data are still not well understood. Specifically, the data can be repetitive in surprising ways, not only semantically but also syntactically and lexically. We present LinguisticLens, a novel interactive visualization tool for making sense of and analyzing syntactic diversity of LLM-generated datasets. LinguisticLens clusters text along syntactic, lexical, and semantic axes. It supports hierarchical visualization of a text dataset, allowing users to quickly scan for an overview and inspect individual examples. The live demo is available at https://shorturl.at/zHOUV.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Visualizing Query Traversals Over Bounding Volume Hierarchies Using Treemaps",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00019",
    "authors": [
      {
        "name": "Abhishek Madan",
        "affiliation": "University of Toronto"
      },
      {
        "name": "Carolina Nobre",
        "affiliation": "University of Toronto"
      }
    ],
    "abstract": "Bounding volume hierarchies (BVHs) are one of the most common spatial data structures in computer graphics. Visualizing ray intersections in these data structures is challenging due to the large number of queries in typical image rendering workloads, the spatial clutter induced by superimposing the tree in a 3D viewport, and the strong tendency of these queries to visit several tree leaves, all of which add a very high dimensionality to the data being visualized. We present a new technique for visualizing ray intersection traversals on BVHs over triangle meshes. Unlike previous approaches which display aggregate traversal costs using a heatmap over the rendered image, we display detailed traversal information about individual queries, using a 3D view of the mesh, a treemap of the BVH, and synchronized highlighting between the two views, along with a pixel grid to select a ray intersection query to view. We demonstrate how this technique elucidates traversal dynamics and tree construction properties, which makes it possible to easily spot algorithmic improvements in these two categories.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Visualizing Similarity of Pathline Dynamics in 2D Flow Fields",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00023",
    "authors": [
      {
        "name": "Baldwin Nsonga",
        "affiliation": "Leipzig University"
      },
      {
        "name": "Gerik Scheuermann",
        "affiliation": "Leipzig University"
      }
    ],
    "abstract": "Even though the analysis of unsteady 2D flow fields is challenging, fluid mechanics experts generally have an intuition on where in the simulation domain specific features are expected. Using this intuition, showing similar regions enables the user to discover flow patterns within the simulation data. When focusing on similarity, a solid mathematical framework for a specific flow pattern is not required. We propose a technique that visualizes similar and dissimilar regions with respect to a region selected by the user. Using infinitesimal strain theory, we capture the strain and rotation progression and therefore the dynamics of fluid parcels along pathlines, which we encode as distributions. We then apply the Jensen\u2013Shannon divergence to compute the (dis)similarity between pathline dynamics originating in a user-defined flow region and the pathline dynamics of the flow field. We validate our method by applying it to two simulation datasets of two-dimensional unsteady flows. Our results show that our approach is suitable for analyzing the similarity of time-dependent flow fields.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Visualizing an Exascale Data Center Digital Twin: Considerations, Challenges and Opportunities",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00012",
    "authors": [
      {
        "name": "Matthias Maiterth",
        "affiliation": "Oak Ridge National Laboratory"
      },
      {
        "name": "Wes Brewer",
        "affiliation": "Oak Ridge National Laboratory"
      },
      {
        "name": "Dane De Wet",
        "affiliation": "Oak Ridge National Laboratory"
      },
      {
        "name": "Scott Greenwood",
        "affiliation": "Oak Ridge National Laboratory"
      },
      {
        "name": "Vineet Kumar",
        "affiliation": "Oak Ridge National Laboratory"
      },
      {
        "name": "Jesse Hines",
        "affiliation": "Oak Ridge National Laboratory"
      },
      {
        "name": "Sedrick Bouknight",
        "affiliation": "Oak Ridge National Laboratory"
      },
      {
        "name": "Zhe Wang",
        "affiliation": "Oak Ridge National Laboratory"
      },
      {
        "name": "Tim Dykes",
        "affiliation": "Hewlett Packard Enterprise"
      },
      {
        "name": "Feiyi Wang",
        "affiliation": "Oak Ridge National Laboratory"
      }
    ],
    "abstract": "Digital twins are an excellent tool to model, visualize, and simulate complex systems, to understand and optimize their operation. In this work, we present the technical challenges of real-time visualization of a digital twin of the Frontier supercomputer.We show the initial prototype and current state of the twin and highlight technical design challenges of visualizing such a large High Performance Computing (HPC) system. The goal is to understand the use of augmented reality as a primary way to extract information and collaborate on digital twins of complex systems. This leverages the spatio-temporal aspect of a 3D representation of a digital twin, with the ability to view historical and real-time telemetry, triggering simulations of a system state and viewing the results, which can be augmented via dashboards for details. Finally, we discuss considerations and opportunities for augmented reality of digital twins of large-scale, parallel computers.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "WUDA: Visualizing and Transforming Rotations in Real-Time with Quaternions and Smart Devices",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00057",
    "authors": [
      {
        "name": "Slobodan Milanko",
        "affiliation": "N/A"
      }
    ],
    "abstract": "The rising popularity of inertial sensing via smart devices is evident, finding use in multidisciplinary applications, such as gesture recognition and image stabilization. Amid the popularity of analyzing raw motion signals and Euler angles, we highlight quaternion rotations as a robust alternative for studying device orientation. Quaternions are a powerful mathematical tool for representing and affecting three-dimensional rotations. Their abstract nature, however, can make it difficult for researchers to visualize the data they provide. Mobile sensing rarely invents practical tools to experiment with this mathematical method. Since visualization effectively communicates data findings, we develop an open-source, real-time app, Wuda, that allows users to observe the orientation of smart devices via inertial sensing. Wuda helps users reduce dimensional complexity and intuitively study quaternion transformations. We demonstrate the practical nature of Wuda in the context of fitness tracking.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Welcome Message from the VIS 2023 General Chairs",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00005",
    "authors": [
      {
        "name": "Tim Dwyer",
        "affiliation": "Monash University, Australia"
      },
      {
        "name": "Sarah Goodwin",
        "affiliation": "Monash University, Australia"
      },
      {
        "name": "Michael Wybrow",
        "affiliation": "Monash University, Australia"
      }
    ],
    "abstract": "We are thrilled this year to welcome the IEEE VIS conference for its first ever visit to Australia. It is, for that matter, the first time for this conference to come to the Southern Hemisphere, as well as its first time in an Asian time zone. We acknowledge Traditional Owners of Narrm, the Boon Wurrung and Wurundjeri Woi Wurrung people of the Kulin nation, on whose land VIS 2023 will be held. We pay our respects to their Elders past, present and emerging. We know Melbourne (Narrm) is a long way from Europe and the US, as we\u2019ve been doing the reverse trip ourselves for many years. But we are grateful to all from those regions who are able to join us, and we are delighted to welcome many new participants from the Asia-Pacific region who are now able to attend in-person for the first time. This sojourn down-under marks a significant milestone for IEEE VIS towards becoming a truly global conference.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "WhaleVis: Visualizing the History of Commercial Whaling",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00028",
    "authors": [
      {
        "name": "Ameya Patil",
        "affiliation": "Computer Science and Engineering, University of Washington, Seattle"
      },
      {
        "name": "Zoe Rand",
        "affiliation": "Quantitative Ecology and Resource Management, University of Washington, Seattle"
      },
      {
        "name": "Trevor Branch",
        "affiliation": "School of Aquatic and Fishery Sciences, University of Washington, Seattle"
      },
      {
        "name": "Leilani Battle",
        "affiliation": "Computer Science and Engineering, University of Washington, Seattle"
      }
    ],
    "abstract": "Whales are an important part of the oceanic ecosystem. Although historic commercial whale hunting a.k.a. whaling has severely threatened whale populations, whale researchers are looking at historical whaling data to inform current whale status and future conservation efforts. To facilitate this, we worked with experts in aquatic and fishery sciences to create WhaleVis\u2014an interactive dashboard for the commercial whaling dataset maintained by the International Whaling Commission (IWC). We characterize key analysis tasks among whale researchers for this database, most important of which is inferring spatial distribution of whale populations over time. In addition to facilitating analysis of whale catches based on the spatio-temporal attributes, we use whaling expedition details to plot the search routes of expeditions. We propose a model of the catch data as a graph, where nodes represent catch locations, and edges represent whaling expedition routes. This model facilitates visual estimation of whale search effort and in turn the spatial distribution of whale populations normalized by the search effort\u2014a well known problem in fisheries research. It further opens up new avenues for graph analysis on the data, including more rigorous computation of spatial distribution of whales normalized by the search effort, and enabling new insight generation. We demonstrate the use of our dashboard through a real life use case.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "What Color Scheme is More Effective in Assisting Readers to Locate Information in a Color-Coded Article?",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00066",
    "authors": [
      {
        "name": "Ho Yin Ng",
        "affiliation": "Pennsylvania State University"
      },
      {
        "name": "Zeyu He",
        "affiliation": "Pennsylvania State University"
      },
      {
        "name": "Ting-Hao \u2018Kenneth\u2019 Huang",
        "affiliation": "Pennsylvania State University"
      }
    ],
    "abstract": "Color coding, a technique assigning specific colors to cluster information types, has proven advantages in aiding human cognitive activities, especially reading and comprehension. The rise of Large Language Models (LLMs) has streamlined document coding, enabling simple automatic text labeling with various schemes. This has the potential to make color-coding more accessible and benefit more users. However, the impact of color choice on information seeking is understudied. We conducted a user study assessing various color schemes\u2019 effectiveness in LLM-coded text documents, standardizing contrast ratios to approximately 5.55:1 across schemes. Participants performed timed information-seeking tasks in color-coded scholarly abstracts. Results showed non-analogous and yellow-inclusive color schemes improved performance, with the latter also being more preferred by participants. These findings can inform better color scheme choices for text annotation. As LLMs advance document coding, we advocate for more research focusing on the \"color\" aspect of color-coding techniques.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "What Exactly is an Insight? A Literature Review",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00027",
    "authors": [
      {
        "name": "Leilani Battle",
        "affiliation": "University of Washington"
      },
      {
        "name": "Alvitta Ottley",
        "affiliation": "Washington University in St. Louis"
      }
    ],
    "abstract": "Insights are often considered the ideal outcome of visual analysis sessions. However, there is no single definition of what an insight is. Some scholars define insights as correlations, while others define them as hypotheses or aha moments. This lack of a clear definition can make it difficult to build visualization tools that effectively support insight discovery. In this paper, we contribute a comprehensive literature review that maps the landscape of existing insight definitions. We summarize key themes regarding how insight is defined, with the goal of helping readers identify which definitions of insight align closely with their research and tool development goals. Based on our review, we also suggest interesting research directions, such as synthesizing a unified formalism for insight and connecting theories of insight to other critical concepts in visualization research.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "What Is the Difference Between a Mountain and a Molehill? Quantifying Semantic Labeling of Visual Features in Line Charts",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00041",
    "authors": [
      {
        "name": "Dennis Bromley",
        "affiliation": "Tableau Research, Seattle, WA, USA"
      },
      {
        "name": "Vidya Setlur",
        "affiliation": "Tableau Research, Palo Alto, CA, USA"
      }
    ],
    "abstract": "Relevant language describing visual features in charts can be useful for authoring captions and summaries about the charts to help with readers\u2019 takeaways. To better understand the interplay between concepts that describe visual features and the semantic relationships among those concepts (e.g., \u2018sharp increase\u2019 vs. \u2018gradual rise\u2019), we conducted a crowdsourced study to collect labels and visual feature pairs for univariate line charts. Using this crowdsourced dataset of labeled visual signatures, this paper proposes a novel method for labeling visual chart features based on combining feature-word distributions with the visual features and the data domain of the charts. These feature-word-topic models identify word associations with similar yet subtle differences in semantics, such as \u2018flat,\u2019 \u2018plateau,\u2019 and \u2018stagnant,\u2019 and descriptors of the visual features, such as \u2018sharp increase,\u2019 \u2018slow climb,\u2019 and \u2018peak.\u2019 Our feature-word-topic model is computed using both a quantified semantics approach and a signal processing-inspired least-errors shape-similarity approach. We finally demonstrate the application of this dataset for annotating charts and generating textual data summaries.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "ZADU: A Python Library for Evaluating the Reliability of Dimensionality Reduction Embeddings",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00048",
    "authors": [
      {
        "name": "Hyeon Jeon",
        "affiliation": "Seoul National University"
      },
      {
        "name": "Aeri Cho",
        "affiliation": "Seoul National University"
      },
      {
        "name": "Jinhwa Jang",
        "affiliation": "Seoul National University"
      },
      {
        "name": "Soohyun Lee",
        "affiliation": "Seoul National University"
      },
      {
        "name": "Jake Hyun",
        "affiliation": "Seoul National University"
      },
      {
        "name": "Hyung-Kwon Ko",
        "affiliation": "KAIST"
      },
      {
        "name": "Jaemin Jo",
        "affiliation": "Sungkyunkwan University"
      },
      {
        "name": "Jinwook Seo",
        "affiliation": "Seoul National University"
      }
    ],
    "abstract": "Dimensionality reduction (DR) techniques inherently distort the original structure of input high-dimensional data, producing imperfect low-dimensional embeddings. Diverse distortion measures have thus been proposed to evaluate the reliability of DR embeddings. However, implementing and executing distortion measures in practice has so far been time-consuming and tedious. To address this issue, we present ZADU, a Python library that provides distortion measures. ZADU is not only easy to install and execute but also enables comprehensive evaluation of DR embeddings through three key features. First, the library covers a wide range of distortion measures. Second, it automatically optimizes the execution of distortion measures, substantially reducing the running time required to execute multiple measures. Last, the library informs how individual points contribute to the overall distortions, facilitating the detailed analysis of DR embeddings. By simulating a real-world scenario of optimizing DR embeddings, we verify that our optimization scheme substantially reduces the time required to execute distortion measures. Finally, as an application of ZADU, we present another library called ZADUVis that allows users to easily create distortion visualizations that depict the extent to which each region of an embedding suffers from distortions.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2024 IEEE Visualization and Visual Analytics (VIS)",
    "title": "Zoomable Level-of-Detail ChartTables for Interpreting Probabilistic Model Outputs for Reactionary Train Delays",
    "publication_year": 2024,
    "doi": "10.1109/VIS55277.2024.00056",
    "authors": [
      {
        "name": "Aidan Slingsby",
        "affiliation": "University of London"
      },
      {
        "name": "Jonathan Hyde",
        "affiliation": "Risk Solutions"
      }
    ],
    "abstract": "\"Reactionary delay\" is a result of the accumulated cascading effects of knock-on train delays which is increasing on UK railways due to increasing utilisation of the railway infrastructure. The chaotic nature of its effects on train lateness is notoriously hard to predict. We use a stochastic Monte-Carto-style simulation of reactionary delay that produces whole distributions of likely reactionary delay and delays this causes. We demonstrate how Zoomable Level-of-Detail ChartTables \u2013 case-by-variable tables where cases are rows, variables are columns, variables are complex composite metrics that incorporate distributions, and cells contain mini-charts that depict these as different levels of detail through zoom interaction \u2013 help interpret whole distributions of model outputs to help understand the causes and effects of reactionary delay, how they inform timetable robustness testing, and how they could be used in other contexts.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "reVISit: Supporting Scalable Evaluation of Interactive Visualizations",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00015",
    "authors": [
      {
        "name": "Yiren Ding",
        "affiliation": "Worcester Polytechnic Institute"
      },
      {
        "name": "Jack Wilburn",
        "affiliation": "University of Utah"
      },
      {
        "name": "Hilson Shrestha",
        "affiliation": "Worcester Polytechnic Institute"
      },
      {
        "name": "Akim Ndlovu",
        "affiliation": "Worcester Polytechnic Institute"
      },
      {
        "name": "Kiran Gadhave",
        "affiliation": "University of Utah"
      },
      {
        "name": "Carolina Nobre",
        "affiliation": "University of Toronto"
      },
      {
        "name": "Alexander Lex",
        "affiliation": "University of Utah"
      },
      {
        "name": "Lane Harrison",
        "affiliation": "Worcester Polytechnic Institute"
      }
    ],
    "abstract": "reVISit is an open-source software toolkit and framework for creating, deploying, and monitoring empirical visualization studies. Running a quality empirical study in visualization can be demanding and resource-intensive, requiring substantial time, cost, and technical expertise from the research team. These challenges are amplified as research norms trend towards more complex and rigorous study methodologies, alongside a growing need to evaluate more complex interactive visualizations. reVISit aims to ameliorate these challenges by introducing a domain-specific language for study set-up, and a series of software components, such as UI elements, behavior provenance, and an experiment monitoring and management interface. Together with interactive or static stimuli provided by the experimenter, these are compiled to a ready-to-deploy web-based experiment. We demonstrate reVISit\u2019s functionality by re-implementing two studies \u2014 a graphical perception task and a more complex, interactive study. reVISit is an open-source community project, available at https://revisit.dev/.",
    "keywords": "N/A"
  },
  {
    "publication_title": "2023 IEEE Visualization and Visual Analytics (VIS)",
    "title": "\u201cTwo Heads are Better than One\u201d: Pair-Interviews for Visualization",
    "publication_year": 2023,
    "doi": "10.1109/VIS54172.2023.00050",
    "authors": [
      {
        "name": "Derya Akbaba",
        "affiliation": "Link\u00f6ping University"
      },
      {
        "name": "Miriah Meyer",
        "affiliation": "Link\u00f6ping University"
      }
    ],
    "abstract": "Visualization research methods help us study how visualization systems are used in complex real-world scenarios. One such widely used method is the interview \u2014 researchers asking participants specific questions to enrich their understanding. In this work, we introduce the pair-interview technique as a method that relies on two interviewers with specific and delineated roles, instead of one. Pair-interviewing focuses on the mechanics of conducting semi-structured interviews as a pair, and complements other existing visualization interview techniques. Based on a synthesis of the experiences and reflections of researchers in four diverse studies who used pair-interviewing, we outline recommendations for when and how to use pair-interviewing within visualization research studies.",
    "keywords": "N/A"
  }
]